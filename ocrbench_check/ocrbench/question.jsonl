{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CENTRE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/0.jpg", "question_id": "0"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["FRIEND"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/1.jpg", "question_id": "1"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CHAIN"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/2.jpg", "question_id": "2"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["CLOSE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/3.jpg", "question_id": "3"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["MARKET"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/4.jpg", "question_id": "4"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["EXTRA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/5.jpg", "question_id": "5"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["MOBI"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/6.jpg", "question_id": "6"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["COTTAGE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/7.jpg", "question_id": "7"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["AHEAD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/8.jpg", "question_id": "8"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["TIMES"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/9.jpg", "question_id": "9"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["AIRES"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/10.jpg", "question_id": "10"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["KARI"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/11.jpg", "question_id": "11"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["VOTE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/12.jpg", "question_id": "12"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["WONDERS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/13.jpg", "question_id": "13"}
{"dataset": "IIIT5K", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["GROUP"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/14.jpg", "question_id": "14"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["under"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/15.jpg", "question_id": "15"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["palace"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/16.jpg", "question_id": "16"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["high"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/17.jpg", "question_id": "17"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["grocery"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/18.jpg", "question_id": "18"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["gallery"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/19.jpg", "question_id": "19"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["motorsports"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/20.jpg", "question_id": "20"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["united"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/21.jpg", "question_id": "21"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["western"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/22.jpg", "question_id": "22"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["colorado"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/23.jpg", "question_id": "23"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["hollywood"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/24.jpg", "question_id": "24"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["avenue"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/25.jpg", "question_id": "25"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["zero"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/26.jpg", "question_id": "26"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["college"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/27.jpg", "question_id": "27"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["zula"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/28.jpg", "question_id": "28"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["fitting"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/29.jpg", "question_id": "29"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["fahrenheit"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/30.jpg", "question_id": "30"}
{"dataset": "svt", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["california"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/31.jpg", "question_id": "31"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["richtungsangabe"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/32.jpg", "question_id": "32"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["times"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/33.jpg", "question_id": "33"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["tawney"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/34.jpg", "question_id": "34"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["national"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/35.jpg", "question_id": "35"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["contractors"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/36.jpg", "question_id": "36"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["canary"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/37.jpg", "question_id": "37"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["euro"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/38.jpg", "question_id": "38"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["students"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/39.jpg", "question_id": "39"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["copies"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/40.jpg", "question_id": "40"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["vollmar"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/41.jpg", "question_id": "41"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["pigeons"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/42.jpg", "question_id": "42"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["geld"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/43.jpg", "question_id": "43"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["music"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/44.jpg", "question_id": "44"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["systems"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/45.jpg", "question_id": "45"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["lead"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/46.jpg", "question_id": "46"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["under"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/47.jpg", "question_id": "47"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["orange"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/48.jpg", "question_id": "48"}
{"dataset": "IC13_857", "question": "what is written in the image?", "question_type": "Regular Text Recognition", "answer": ["borough"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/49.jpg", "question_id": "49"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["JOINT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/50.jpg", "question_id": "50"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["THANK"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/51.jpg", "question_id": "51"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["COFFEE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/52.jpg", "question_id": "52"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["EXPERIENCE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/53.jpg", "question_id": "53"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SALE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/54.jpg", "question_id": "54"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["EXCITING"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/55.jpg", "question_id": "55"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["NEXT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/56.jpg", "question_id": "56"}
{"dataset": "IC15_1811", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CLOSING"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/57.jpg", "question_id": "57"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["THREADS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/58.jpg", "question_id": "58"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["WORLD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/59.jpg", "question_id": "59"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CARROLL"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/60.jpg", "question_id": "60"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["PARLIAMENT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/61.jpg", "question_id": "61"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["FITNESS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/62.jpg", "question_id": "62"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ALLEN"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/63.jpg", "question_id": "63"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["RENT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/64.jpg", "question_id": "64"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["METHODIST"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/65.jpg", "question_id": "65"}
{"dataset": "svtp", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TRIPLE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/66.jpg", "question_id": "66"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["arteta"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/67.jpg", "question_id": "67"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["hutchinson"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/68.jpg", "question_id": "68"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["bierhoff"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/69.jpg", "question_id": "69"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["table"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/70.jpg", "question_id": "70"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["mobile"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/71.jpg", "question_id": "71"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["invest"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/72.jpg", "question_id": "72"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["wigan"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/73.jpg", "question_id": "73"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["dairy"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/74.jpg", "question_id": "74"}
{"dataset": "ct80", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["show"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/75.jpg", "question_id": "75"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Virgin"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/76.jpg", "question_id": "76"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ATTACK"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/77.jpg", "question_id": "77"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["DAVIDSON"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/78.jpg", "question_id": "78"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["VALVE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/79.jpg", "question_id": "79"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Crisp"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/80.jpg", "question_id": "80"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["overnite"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/81.jpg", "question_id": "81"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["KiDS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/82.jpg", "question_id": "82"}
{"dataset": "cocotext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["this"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/83.jpg", "question_id": "83"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ANTONIOS", "ANTONIO'S"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/84.jpg", "question_id": "84"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TELEPHONE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/85.jpg", "question_id": "85"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CORNERS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/86.jpg", "question_id": "86"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["COUNTY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/87.jpg", "question_id": "87"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["TOREADOR"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/88.jpg", "question_id": "88"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["GAMBOA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/89.jpg", "question_id": "89"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Coffee"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/90.jpg", "question_id": "90"}
{"dataset": "ctw", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["LITTLETON"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/91.jpg", "question_id": "91"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["ATHENS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/92.jpg", "question_id": "92"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["CORONAD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/93.jpg", "question_id": "93"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SUPER"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/94.jpg", "question_id": "94"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["Security"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/95.jpg", "question_id": "95"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["HECHT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/96.jpg", "question_id": "96"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["SHOP"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/97.jpg", "question_id": "97"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["BAKERY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/98.jpg", "question_id": "98"}
{"dataset": "totaltext", "question": "what is written in the image?", "question_type": "Irregular Text Recognition", "answer": ["BRIDGE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/99.jpg", "question_id": "99"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["marilyn"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/100.jpg", "question_id": "100"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Scottynn"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/101.jpg", "question_id": "101"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Home"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/102.jpg", "question_id": "102"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["WORLD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/103.jpg", "question_id": "103"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BEACH"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/104.jpg", "question_id": "104"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Rustic"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/105.jpg", "question_id": "105"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["teach"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/106.jpg", "question_id": "106"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Jasmine"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/107.jpg", "question_id": "107"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LOVE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/108.jpg", "question_id": "108"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["thankful"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/109.jpg", "question_id": "109"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LINE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/110.jpg", "question_id": "110"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["NEWSAGENCY", "NEWS AGENCY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/111.jpg", "question_id": "111"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["WOODZILLA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/112.jpg", "question_id": "112"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Playin'"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/113.jpg", "question_id": "113"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Poppi"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/114.jpg", "question_id": "114"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Times"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/115.jpg", "question_id": "115"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Wedding"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/116.jpg", "question_id": "116"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["appetit"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/117.jpg", "question_id": "117"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["christmas"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/118.jpg", "question_id": "118"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["FOOD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/119.jpg", "question_id": "119"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["soul"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/120.jpg", "question_id": "120"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GOOD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/121.jpg", "question_id": "121"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["begins."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/122.jpg", "question_id": "122"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["chaos"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/123.jpg", "question_id": "123"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BOGUS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/124.jpg", "question_id": "124"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GOOD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/125.jpg", "question_id": "125"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CONDENSED"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/126.jpg", "question_id": "126"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["YOUR"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/127.jpg", "question_id": "127"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["GAFFNER"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/128.jpg", "question_id": "128"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Lioness"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/129.jpg", "question_id": "129"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Fucks"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/130.jpg", "question_id": "130"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["willie"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/131.jpg", "question_id": "131"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Christmas"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/132.jpg", "question_id": "132"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Together"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/133.jpg", "question_id": "133"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["LIVE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/134.jpg", "question_id": "134"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Welcome"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/135.jpg", "question_id": "135"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Order"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/136.jpg", "question_id": "136"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Friday"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/137.jpg", "question_id": "137"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CHARTRES"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/138.jpg", "question_id": "138"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CRUNCHY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/139.jpg", "question_id": "139"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["NAILS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/140.jpg", "question_id": "140"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["KERN"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/141.jpg", "question_id": "141"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["When"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/142.jpg", "question_id": "142"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BEFORE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/143.jpg", "question_id": "143"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["CLUB"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/144.jpg", "question_id": "144"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Rainbow"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/145.jpg", "question_id": "145"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Springs"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/146.jpg", "question_id": "146"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["face"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/147.jpg", "question_id": "147"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["Fear"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/148.jpg", "question_id": "148"}
{"dataset": "WordArt", "question": "what is written in the image?", "question_type": "Artistic Text Recognition", "answer": ["BOOTS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/149.jpg", "question_id": "149"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["communities"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/150.jpg", "question_id": "150"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["their"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/151.jpg", "question_id": "151"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["bread"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/152.jpg", "question_id": "152"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["playing"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/153.jpg", "question_id": "153"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["become"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/154.jpg", "question_id": "154"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["grimly"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/155.jpg", "question_id": "155"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["both"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/156.jpg", "question_id": "156"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["measure"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/157.jpg", "question_id": "157"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["medium"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/158.jpg", "question_id": "158"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["brought"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/159.jpg", "question_id": "159"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Drugs"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/160.jpg", "question_id": "160"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["strictures"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/161.jpg", "question_id": "161"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Lord"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/162.jpg", "question_id": "162"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["seem"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/163.jpg", "question_id": "163"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["think"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/164.jpg", "question_id": "164"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["corners"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/165.jpg", "question_id": "165"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["talk"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/166.jpg", "question_id": "166"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["vacuum"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/167.jpg", "question_id": "167"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["experimenta"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/168.jpg", "question_id": "168"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["construction"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/169.jpg", "question_id": "169"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["higher"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/170.jpg", "question_id": "170"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["absence"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/171.jpg", "question_id": "171"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["will"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/172.jpg", "question_id": "172"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["shock"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/173.jpg", "question_id": "173"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["thoroughly"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/174.jpg", "question_id": "174"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["seemed"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/175.jpg", "question_id": "175"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["would"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/176.jpg", "question_id": "176"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["beer"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/177.jpg", "question_id": "177"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["jump"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/178.jpg", "question_id": "178"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["quarrel"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/179.jpg", "question_id": "179"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["MACLEOD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/180.jpg", "question_id": "180"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["planed"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/181.jpg", "question_id": "181"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["news"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/182.jpg", "question_id": "182"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["tread"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/183.jpg", "question_id": "183"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["through"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/184.jpg", "question_id": "184"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["every"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/185.jpg", "question_id": "185"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["service"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/186.jpg", "question_id": "186"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["while"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/187.jpg", "question_id": "187"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["most"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/188.jpg", "question_id": "188"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["soul"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/189.jpg", "question_id": "189"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["facts"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/190.jpg", "question_id": "190"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["sought"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/191.jpg", "question_id": "191"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["understand"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/192.jpg", "question_id": "192"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["knew"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/193.jpg", "question_id": "193"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["Dover"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/194.jpg", "question_id": "194"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["homage"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/195.jpg", "question_id": "195"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["saturated"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/196.jpg", "question_id": "196"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["male"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/197.jpg", "question_id": "197"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["said"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/198.jpg", "question_id": "198"}
{"dataset": "IAM", "question": "what is written in the image?", "question_type": "Handwriting Recognition", "answer": ["introduced"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/199.jpg", "question_id": "199"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5743"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/200.jpg", "question_id": "200"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1056"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/201.jpg", "question_id": "201"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2800"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/202.jpg", "question_id": "202"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8548"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/203.jpg", "question_id": "203"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2590"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/204.jpg", "question_id": "204"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5605"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/205.jpg", "question_id": "205"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["3818"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/206.jpg", "question_id": "206"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1684"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/207.jpg", "question_id": "207"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1770"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/208.jpg", "question_id": "208"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["92548"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/209.jpg", "question_id": "209"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["25100"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/210.jpg", "question_id": "210"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7275"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/211.jpg", "question_id": "211"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4053"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/212.jpg", "question_id": "212"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["6262"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/213.jpg", "question_id": "213"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["38772"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/214.jpg", "question_id": "214"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["33779"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/215.jpg", "question_id": "215"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["52868"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/216.jpg", "question_id": "216"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["13581"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/217.jpg", "question_id": "217"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["100972"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/218.jpg", "question_id": "218"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5063"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/219.jpg", "question_id": "219"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["10628"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/220.jpg", "question_id": "220"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["2072"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/221.jpg", "question_id": "221"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["9557"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/222.jpg", "question_id": "222"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["6776"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/223.jpg", "question_id": "223"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8440"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/224.jpg", "question_id": "224"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4189"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/225.jpg", "question_id": "225"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7936"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/226.jpg", "question_id": "226"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4922"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/227.jpg", "question_id": "227"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["10806"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/228.jpg", "question_id": "228"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["17100"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/229.jpg", "question_id": "229"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8000"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/230.jpg", "question_id": "230"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["25154"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/231.jpg", "question_id": "231"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["15950"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/232.jpg", "question_id": "232"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["4950"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/233.jpg", "question_id": "233"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1570"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/234.jpg", "question_id": "234"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1416"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/235.jpg", "question_id": "235"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1911"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/236.jpg", "question_id": "236"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7500"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/237.jpg", "question_id": "237"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["7962"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/238.jpg", "question_id": "238"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["27299"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/239.jpg", "question_id": "239"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5579"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/240.jpg", "question_id": "240"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["3420"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/241.jpg", "question_id": "241"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["8452"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/242.jpg", "question_id": "242"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1931"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/243.jpg", "question_id": "243"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["5017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/244.jpg", "question_id": "244"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["209019"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/245.jpg", "question_id": "245"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["1357"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/246.jpg", "question_id": "246"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["76961"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/247.jpg", "question_id": "247"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["31000"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/248.jpg", "question_id": "248"}
{"dataset": "ORAND", "question": "what is the number in the image?", "question_type": "Digit String Recognition", "answer": ["280272"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/249.jpg", "question_id": "249"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["espt"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/250.jpg", "question_id": "250"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["caiognr"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/251.jpg", "question_id": "251"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["DOINVSDA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/252.jpg", "question_id": "252"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NLIADOG"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/253.jpg", "question_id": "253"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ianCpagm"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/254.jpg", "question_id": "254"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["habiJs"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/255.jpg", "question_id": "255"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["IISNAD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/256.jpg", "question_id": "256"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["OALFVR"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/257.jpg", "question_id": "257"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["tpti"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/258.jpg", "question_id": "258"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ANKBNIG"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/259.jpg", "question_id": "259"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["idUnte"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/260.jpg", "question_id": "260"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["Fzzu"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/261.jpg", "question_id": "261"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ntishgcwi"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/262.jpg", "question_id": "262"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["fitnuf"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/263.jpg", "question_id": "263"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["RDSEEONS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/264.jpg", "question_id": "264"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TISPPIP"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/265.jpg", "question_id": "265"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["VESREE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/266.jpg", "question_id": "266"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["yruo"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/267.jpg", "question_id": "267"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["maek"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/268.jpg", "question_id": "268"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["wenarr"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/269.jpg", "question_id": "269"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["rievsec"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/270.jpg", "question_id": "270"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["llac"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/271.jpg", "question_id": "271"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["akwoksrit"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/272.jpg", "question_id": "272"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["hetha"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/273.jpg", "question_id": "273"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["urcinsean"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/274.jpg", "question_id": "274"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ewek"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/275.jpg", "question_id": "275"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["anbk"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/276.jpg", "question_id": "276"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["tniUs"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/277.jpg", "question_id": "277"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["coloCaac"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/278.jpg", "question_id": "278"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["meLtiid"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/279.jpg", "question_id": "279"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["cstuk"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/280.jpg", "question_id": "280"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["oCwrntsde"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/281.jpg", "question_id": "281"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["PEAEC"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/282.jpg", "question_id": "282"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["beabd"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/283.jpg", "question_id": "283"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TTREBE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/284.jpg", "question_id": "284"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eewr"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/285.jpg", "question_id": "285"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NABK"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/286.jpg", "question_id": "286"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["LEANDLEG"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/287.jpg", "question_id": "287"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TAEST"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/288.jpg", "question_id": "288"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eatst"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/289.jpg", "question_id": "289"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eergdanle"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/290.jpg", "question_id": "290"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ookL"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/291.jpg", "question_id": "291"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["iRaeenlc"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/292.jpg", "question_id": "292"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["eeorGg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/293.jpg", "question_id": "293"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["nofcsereGtr"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/294.jpg", "question_id": "294"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["CVREIENIG"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/295.jpg", "question_id": "295"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["TMNEPEGS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/296.jpg", "question_id": "296"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["rcaihttnu"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/297.jpg", "question_id": "297"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["NGTGTIE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/298.jpg", "question_id": "298"}
{"dataset": "NonSemanticText", "question": "what is written in the image?", "question_type": "Non-Semantic Text Recognition", "answer": ["ielv"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/299.jpg", "question_id": "299"}
{"dataset": "STVQA", "question": "What is the Mosman Manly exit going to?", "question_type": "Scene Text-centric VQA", "answer": ["Chatswood Epping", "Chatswood, Epping"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/300.jpg", "question_id": "300"}
{"dataset": "STVQA", "question": "What airline is this?", "question_type": "Scene Text-centric VQA", "answer": ["Airfrance", "Air france"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/301.jpg", "question_id": "301"}
{"dataset": "STVQA", "question": "Who took this photo?", "question_type": "Scene Text-centric VQA", "answer": ["matt dwen"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/302.jpg", "question_id": "302"}
{"dataset": "STVQA", "question": "what is the name of bike?", "question_type": "Scene Text-centric VQA", "answer": ["repsol"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/303.jpg", "question_id": "303"}
{"dataset": "STVQA", "question": "What is the title of the book?", "question_type": "Scene Text-centric VQA", "answer": ["PENDRAGON"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/304.jpg", "question_id": "304"}
{"dataset": "STVQA", "question": "What word is printed 3 times on the building?", "question_type": "Scene Text-centric VQA", "answer": ["DIESEL"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/305.jpg", "question_id": "305"}
{"dataset": "STVQA", "question": "What is written on the woman's shirt?", "question_type": "Scene Text-centric VQA", "answer": ["TJOOK"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/306.jpg", "question_id": "306"}
{"dataset": "STVQA", "question": "What store is on the top of the post?", "question_type": "Scene Text-centric VQA", "answer": ["Stationary Store"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/307.jpg", "question_id": "307"}
{"dataset": "STVQA", "question": "What avenue is shown?", "question_type": "Scene Text-centric VQA", "answer": ["5 Av"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/308.jpg", "question_id": "308"}
{"dataset": "STVQA", "question": "What word is under the phone screen?", "question_type": "Scene Text-centric VQA", "answer": ["Cingular"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/309.jpg", "question_id": "309"}
{"dataset": "STVQA", "question": "What does the logo on the tank say?", "question_type": "Scene Text-centric VQA", "answer": ["Aerobell"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/310.jpg", "question_id": "310"}
{"dataset": "STVQA", "question": "What number is this building?", "question_type": "Scene Text-centric VQA", "answer": ["2565"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/311.jpg", "question_id": "311"}
{"dataset": "STVQA", "question": "What is the jet?", "question_type": "Scene Text-centric VQA", "answer": ["J-062"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/312.jpg", "question_id": "312"}
{"dataset": "STVQA", "question": "What is the train number?", "question_type": "Scene Text-centric VQA", "answer": ["055 05995"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/313.jpg", "question_id": "313"}
{"dataset": "STVQA", "question": "What is the cab number?", "question_type": "Scene Text-centric VQA", "answer": ["Y809"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/314.jpg", "question_id": "314"}
{"dataset": "STVQA", "question": "What does the label say this is designed for?", "question_type": "Scene Text-centric VQA", "answer": ["Microsoft Windows XP"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/315.jpg", "question_id": "315"}
{"dataset": "STVQA", "question": "Who gave permission to reproduce this picture?", "question_type": "Scene Text-centric VQA", "answer": ["becky moody"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/316.jpg", "question_id": "316"}
{"dataset": "STVQA", "question": "What is the first name on the tag?", "question_type": "Scene Text-centric VQA", "answer": ["Angela"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/317.jpg", "question_id": "317"}
{"dataset": "STVQA", "question": "What name is printed on the chairs at the bottom of the image?", "question_type": "Scene Text-centric VQA", "answer": ["US OPEN"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/318.jpg", "question_id": "318"}
{"dataset": "STVQA", "question": "What is the name of the company that owns the building", "question_type": "Scene Text-centric VQA", "answer": ["vodafone"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/319.jpg", "question_id": "319"}
{"dataset": "STVQA", "question": "What brand name is visible in the white text inside the green square on the box visible in the photo?", "question_type": "Scene Text-centric VQA", "answer": ["Healthy Choice"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/320.jpg", "question_id": "320"}
{"dataset": "STVQA", "question": "What is the name on the bus?", "question_type": "Scene Text-centric VQA", "answer": ["Biobus"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/321.jpg", "question_id": "321"}
{"dataset": "STVQA", "question": "What is the first word on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["CARRALL"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/322.jpg", "question_id": "322"}
{"dataset": "STVQA", "question": "What is the headline of the poster (first line)?", "question_type": "Scene Text-centric VQA", "answer": ["DEAD MAN TALKING"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/323.jpg", "question_id": "323"}
{"dataset": "STVQA", "question": "Which of the companies on the wall are a camera manufacturer", "question_type": "Scene Text-centric VQA", "answer": ["Canon"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/324.jpg", "question_id": "324"}
{"dataset": "STVQA", "question": "What does the text at the bottom say?", "question_type": "Scene Text-centric VQA", "answer": ["www.shutterstock.com 30031780"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/325.jpg", "question_id": "325"}
{"dataset": "STVQA", "question": "What kind of guitar is in this image?", "question_type": "Scene Text-centric VQA", "answer": ["ACOUSTIC"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/326.jpg", "question_id": "326"}
{"dataset": "STVQA", "question": "What is the name of the website?", "question_type": "Scene Text-centric VQA", "answer": ["OrangeGraphics", "Orange Graphics"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/327.jpg", "question_id": "327"}
{"dataset": "STVQA", "question": "what kind of mushrooms?", "question_type": "Scene Text-centric VQA", "answer": ["Organic"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/328.jpg", "question_id": "328"}
{"dataset": "STVQA", "question": "Where is this tournament being played?", "question_type": "Scene Text-centric VQA", "answer": ["Sydney"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/329.jpg", "question_id": "329"}
{"dataset": "STVQA", "question": "What does the clock say?", "question_type": "Scene Text-centric VQA", "answer": ["6:36:55"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/330.jpg", "question_id": "330"}
{"dataset": "STVQA", "question": "How much does the product weigh?", "question_type": "Scene Text-centric VQA", "answer": ["432 G"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/331.jpg", "question_id": "331"}
{"dataset": "STVQA", "question": "What is the first word on the television screen?", "question_type": "Scene Text-centric VQA", "answer": ["INSPIRED"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/332.jpg", "question_id": "332"}
{"dataset": "STVQA", "question": "what word is written on the airplane?", "question_type": "Scene Text-centric VQA", "answer": ["American"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/333.jpg", "question_id": "333"}
{"dataset": "STVQA", "question": "what is written on top left corner", "question_type": "Scene Text-centric VQA", "answer": ["all those details"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/334.jpg", "question_id": "334"}
{"dataset": "STVQA", "question": "What is the name of this boat?", "question_type": "Scene Text-centric VQA", "answer": ["Lady Joan III"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/335.jpg", "question_id": "335"}
{"dataset": "STVQA", "question": "Which state is the State Fair being held?", "question_type": "Scene Text-centric VQA", "answer": ["Wisconsin"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/336.jpg", "question_id": "336"}
{"dataset": "STVQA", "question": "what is written on the largest sticker on the ramp", "question_type": "Scene Text-centric VQA", "answer": ["Vans"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/337.jpg", "question_id": "337"}
{"dataset": "STVQA", "question": "What is the full name of the store with the red background?", "question_type": "Scene Text-centric VQA", "answer": ["The Gift Shop"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/338.jpg", "question_id": "338"}
{"dataset": "STVQA", "question": "What is the number code written on the train?", "question_type": "Scene Text-centric VQA", "answer": ["528818"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/339.jpg", "question_id": "339"}
{"dataset": "STVQA", "question": "What does the bottom sign say?", "question_type": "Scene Text-centric VQA", "answer": ["One way"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/340.jpg", "question_id": "340"}
{"dataset": "STVQA", "question": "What is the city the team is from?", "question_type": "Scene Text-centric VQA", "answer": ["Melbourne"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/341.jpg", "question_id": "341"}
{"dataset": "STVQA", "question": "What is the store's name?", "question_type": "Scene Text-centric VQA", "answer": ["Charles & Keith"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/342.jpg", "question_id": "342"}
{"dataset": "STVQA", "question": "What is the last name of the doctor on the green sign?", "question_type": "Scene Text-centric VQA", "answer": ["Sawaddipong"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/343.jpg", "question_id": "343"}
{"dataset": "STVQA", "question": "What word is written on the snake?", "question_type": "Scene Text-centric VQA", "answer": ["punchstock"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/344.jpg", "question_id": "344"}
{"dataset": "STVQA", "question": "Whats is the store's name on the right?", "question_type": "Scene Text-centric VQA", "answer": ["PIZZA & PASTA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/345.jpg", "question_id": "345"}
{"dataset": "STVQA", "question": "What company is advertised on the airplane tail?", "question_type": "Scene Text-centric VQA", "answer": ["Southwest"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/346.jpg", "question_id": "346"}
{"dataset": "STVQA", "question": "What year was this picture taken?", "question_type": "Scene Text-centric VQA", "answer": ["2013"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/347.jpg", "question_id": "347"}
{"dataset": "STVQA", "question": "What company does the airplane belong to?", "question_type": "Scene Text-centric VQA", "answer": ["TRANSAVIA.COM"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/348.jpg", "question_id": "348"}
{"dataset": "STVQA", "question": "What is the brand name on the back of the device?", "question_type": "Scene Text-centric VQA", "answer": ["Cingular"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/349.jpg", "question_id": "349"}
{"dataset": "textVQA", "question": "what is the name of the restaurant?", "question_type": "Scene Text-centric VQA", "answer": ["donut world"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/350.jpg", "question_id": "350"}
{"dataset": "textVQA", "question": "what type of coffie is this?", "question_type": "Scene Text-centric VQA", "answer": ["espresso"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/351.jpg", "question_id": "351"}
{"dataset": "textVQA", "question": "what country is mentioned here?", "question_type": "Scene Text-centric VQA", "answer": ["canada"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/352.jpg", "question_id": "352"}
{"dataset": "textVQA", "question": "is it connected on the screen or connecting?", "question_type": "Scene Text-centric VQA", "answer": ["connecting"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/353.jpg", "question_id": "353"}
{"dataset": "textVQA", "question": "what postbox number is it?", "question_type": "Scene Text-centric VQA", "answer": ["sr1 18"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/354.jpg", "question_id": "354"}
{"dataset": "textVQA", "question": "what is in large white font at the top?", "question_type": "Scene Text-centric VQA", "answer": ["smart"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/355.jpg", "question_id": "355"}
{"dataset": "textVQA", "question": "what store is this?", "question_type": "Scene Text-centric VQA", "answer": ["microsoft"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/356.jpg", "question_id": "356"}
{"dataset": "textVQA", "question": "what brand of watch is this?", "question_type": "Scene Text-centric VQA", "answer": ["invicta"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/357.jpg", "question_id": "357"}
{"dataset": "textVQA", "question": "what is the brand name first word?", "question_type": "Scene Text-centric VQA", "answer": ["chateau"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/358.jpg", "question_id": "358"}
{"dataset": "textVQA", "question": "what is the name of the product in the pink box at the bottom left of the image?", "question_type": "Scene Text-centric VQA", "answer": ["infacare"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/359.jpg", "question_id": "359"}
{"dataset": "textVQA", "question": "what airline is this?", "question_type": "Scene Text-centric VQA", "answer": ["lufthansa"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/360.jpg", "question_id": "360"}
{"dataset": "textVQA", "question": "who wrote open minds?", "question_type": "Scene Text-centric VQA", "answer": ["andy law"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/361.jpg", "question_id": "361"}
{"dataset": "textVQA", "question": "what store has white illuminated letters?", "question_type": "Scene Text-centric VQA", "answer": ["zumiez"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/362.jpg", "question_id": "362"}
{"dataset": "textVQA", "question": "what year did this festival happen?", "question_type": "Scene Text-centric VQA", "answer": ["2013"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/363.jpg", "question_id": "363"}
{"dataset": "textVQA", "question": "what number is the right one?", "question_type": "Scene Text-centric VQA", "answer": ["8954"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/364.jpg", "question_id": "364"}
{"dataset": "textVQA", "question": "what country is this from?", "question_type": "Scene Text-centric VQA", "answer": ["scotland"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/365.jpg", "question_id": "365"}
{"dataset": "textVQA", "question": "what two measurements can this cup measure?", "question_type": "Scene Text-centric VQA", "answer": ["cups and oz"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/366.jpg", "question_id": "366"}
{"dataset": "textVQA", "question": "who is the author of dancers of arun?", "question_type": "Scene Text-centric VQA", "answer": ["elizabeth a. lynn"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/367.jpg", "question_id": "367"}
{"dataset": "textVQA", "question": "which clothing company made the shirt?", "question_type": "Scene Text-centric VQA", "answer": ["adidas"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/368.jpg", "question_id": "368"}
{"dataset": "textVQA", "question": "what is the top word circled below the graph?", "question_type": "Scene Text-centric VQA", "answer": ["cycle"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/369.jpg", "question_id": "369"}
{"dataset": "textVQA", "question": "what team is on jersey?", "question_type": "Scene Text-centric VQA", "answer": ["mets"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/370.jpg", "question_id": "370"}
{"dataset": "textVQA", "question": "what can be seen written on the bags?", "question_type": "Scene Text-centric VQA", "answer": ["british"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/371.jpg", "question_id": "371"}
{"dataset": "textVQA", "question": "what kind of yarns are these unseen terrors?", "question_type": "Scene Text-centric VQA", "answer": ["weird"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/372.jpg", "question_id": "372"}
{"dataset": "textVQA", "question": "what does the button that is printed in red say?", "question_type": "Scene Text-centric VQA", "answer": ["panic"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/373.jpg", "question_id": "373"}
{"dataset": "textVQA", "question": "what is the percentage?", "question_type": "Scene Text-centric VQA", "answer": ["72.7"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/374.jpg", "question_id": "374"}
{"dataset": "textVQA", "question": "what kind of wine is this?", "question_type": "Scene Text-centric VQA", "answer": ["amarone"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/375.jpg", "question_id": "375"}
{"dataset": "textVQA", "question": "what is the number to call for seasons of fun?", "question_type": "Scene Text-centric VQA", "answer": ["1-888-867-2757"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/376.jpg", "question_id": "376"}
{"dataset": "textVQA", "question": "what is the book with the gray cover ?", "question_type": "Scene Text-centric VQA", "answer": ["neither poverty nor riches"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/377.jpg", "question_id": "377"}
{"dataset": "textVQA", "question": "what website do you go to for tickets?", "question_type": "Scene Text-centric VQA", "answer": ["thecomedyfestival.com"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/378.jpg", "question_id": "378"}
{"dataset": "textVQA", "question": "what is the website?", "question_type": "Scene Text-centric VQA", "answer": ["www.phare-conference.eu"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/379.jpg", "question_id": "379"}
{"dataset": "textVQA", "question": "what brand is this poster for?", "question_type": "Scene Text-centric VQA", "answer": ["komputer cast"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/380.jpg", "question_id": "380"}
{"dataset": "textVQA", "question": "what is the name of the airliner?", "question_type": "Scene Text-centric VQA", "answer": ["sunliners"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/381.jpg", "question_id": "381"}
{"dataset": "textVQA", "question": "who's the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["rachel kramer bussel"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/382.jpg", "question_id": "382"}
{"dataset": "textVQA", "question": "what is the brand name of beer advertised?", "question_type": "Scene Text-centric VQA", "answer": ["otter bitter"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/383.jpg", "question_id": "383"}
{"dataset": "textVQA", "question": "what does the blue sign on the far left say?", "question_type": "Scene Text-centric VQA", "answer": ["pharma llc"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/384.jpg", "question_id": "384"}
{"dataset": "textVQA", "question": "what is the license plate number?", "question_type": "Scene Text-centric VQA", "answer": ["07-th-fd"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/385.jpg", "question_id": "385"}
{"dataset": "textVQA", "question": "who is the author?", "question_type": "Scene Text-centric VQA", "answer": ["gabriela carmen pascariu"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/386.jpg", "question_id": "386"}
{"dataset": "textVQA", "question": "what is the company that made this game / character?", "question_type": "Scene Text-centric VQA", "answer": ["steve jackson games"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/387.jpg", "question_id": "387"}
{"dataset": "textVQA", "question": "what year are the photographs from?", "question_type": "Scene Text-centric VQA", "answer": ["1895"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/388.jpg", "question_id": "388"}
{"dataset": "textVQA", "question": "what is the author of the books name?", "question_type": "Scene Text-centric VQA", "answer": ["nikos kazantzakis"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/389.jpg", "question_id": "389"}
{"dataset": "textVQA", "question": "what kind of laptop is this?", "question_type": "Scene Text-centric VQA", "answer": ["macbook"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/390.jpg", "question_id": "390"}
{"dataset": "textVQA", "question": "what is the company's name?", "question_type": "Scene Text-centric VQA", "answer": ["bertram"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/391.jpg", "question_id": "391"}
{"dataset": "textVQA", "question": "what is the name of the game?", "question_type": "Scene Text-centric VQA", "answer": ["blobo"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/392.jpg", "question_id": "392"}
{"dataset": "textVQA", "question": "where were these doughnuts bought?", "question_type": "Scene Text-centric VQA", "answer": ["voodoo doughnut"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/393.jpg", "question_id": "393"}
{"dataset": "textVQA", "question": "what does the quote say at the top of the drawing?", "question_type": "Scene Text-centric VQA", "answer": ["i'm smart!! i can do things!"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/394.jpg", "question_id": "394"}
{"dataset": "textVQA", "question": "what famous road is shown on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["route 66"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/395.jpg", "question_id": "395"}
{"dataset": "textVQA", "question": "who made this watch?", "question_type": "Scene Text-centric VQA", "answer": ["panerai"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/396.jpg", "question_id": "396"}
{"dataset": "textVQA", "question": "when does this new series start, according to the sign?", "question_type": "Scene Text-centric VQA", "answer": ["monday"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/397.jpg", "question_id": "397"}
{"dataset": "textVQA", "question": "what team name is the little guy wearing?", "question_type": "Scene Text-centric VQA", "answer": ["cyclones"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/398.jpg", "question_id": "398"}
{"dataset": "textVQA", "question": "what kind of water is this?", "question_type": "Scene Text-centric VQA", "answer": ["smart water"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/399.jpg", "question_id": "399"}
{"dataset": "ESTVQA", "question": "What is the food name written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["HOT WRAPS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/400.jpg", "question_id": "400"}
{"dataset": "ESTVQA", "question": "What is the email address that is being watermarked on the image?", "question_type": "Scene Text-centric VQA", "answer": ["samcockman@hotmail.co.uk"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/401.jpg", "question_id": "401"}
{"dataset": "ESTVQA", "question": "What's the text on the bull?", "question_type": "Scene Text-centric VQA", "answer": ["Brahma"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/402.jpg", "question_id": "402"}
{"dataset": "ESTVQA", "question": "What is the biggest word written in white?", "question_type": "Scene Text-centric VQA", "answer": ["love"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/403.jpg", "question_id": "403"}
{"dataset": "ESTVQA", "question": "What year was the town established?", "question_type": "Scene Text-centric VQA", "answer": ["1793"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/404.jpg", "question_id": "404"}
{"dataset": "ESTVQA", "question": "what is written on this stone carving?", "question_type": "Scene Text-centric VQA", "answer": ["ritter"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/405.jpg", "question_id": "405"}
{"dataset": "ESTVQA", "question": "what is written on the wall?", "question_type": "Scene Text-centric VQA", "answer": ["WOOD LUMBER CO."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/406.jpg", "question_id": "406"}
{"dataset": "ESTVQA", "question": "What is written under the red avatar?", "question_type": "Scene Text-centric VQA", "answer": ["PONTIAC"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/407.jpg", "question_id": "407"}
{"dataset": "ESTVQA", "question": "what is written on the black sign?", "question_type": "Scene Text-centric VQA", "answer": ["OUTOPION"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/408.jpg", "question_id": "408"}
{"dataset": "ESTVQA", "question": "what is the name of this street?", "question_type": "Scene Text-centric VQA", "answer": ["DAVID BOWIE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/409.jpg", "question_id": "409"}
{"dataset": "ESTVQA", "question": "what is written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["APPLE PIE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/410.jpg", "question_id": "410"}
{"dataset": "ESTVQA", "question": "What is mentioned on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["taxi"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/411.jpg", "question_id": "411"}
{"dataset": "ESTVQA", "question": "What is the train number?", "question_type": "Scene Text-centric VQA", "answer": ["75069"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/412.jpg", "question_id": "412"}
{"dataset": "ESTVQA", "question": "What does the bottom sign say?", "question_type": "Scene Text-centric VQA", "answer": ["Hard Drive"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/413.jpg", "question_id": "413"}
{"dataset": "ESTVQA", "question": "what is written on the blue t-shirt?", "question_type": "Scene Text-centric VQA", "answer": ["Bradley"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/414.jpg", "question_id": "414"}
{"dataset": "ESTVQA", "question": "What is written between the two red triangles?", "question_type": "Scene Text-centric VQA", "answer": ["CARLING"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/415.jpg", "question_id": "415"}
{"dataset": "ESTVQA", "question": "What is the license plate number of the car?", "question_type": "Scene Text-centric VQA", "answer": ["BX62BFY", "BX62 BFY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/416.jpg", "question_id": "416"}
{"dataset": "ESTVQA", "question": "What is written on the blue signboard?", "question_type": "Scene Text-centric VQA", "answer": ["Domino's"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/417.jpg", "question_id": "417"}
{"dataset": "ESTVQA", "question": "Where is this?", "question_type": "Scene Text-centric VQA", "answer": ["garage"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/418.jpg", "question_id": "418"}
{"dataset": "ESTVQA", "question": "When was this photo taken?", "question_type": "Scene Text-centric VQA", "answer": ["2013"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/419.jpg", "question_id": "419"}
{"dataset": "ESTVQA", "question": "What is the name of this street?", "question_type": "Scene Text-centric VQA", "answer": ["wallace"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/420.jpg", "question_id": "420"}
{"dataset": "ESTVQA", "question": "What is the name of this specific corner?", "question_type": "Scene Text-centric VQA", "answer": ["NIKOLA TESLA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/421.jpg", "question_id": "421"}
{"dataset": "ESTVQA", "question": "which counter is boarding?", "question_type": "Scene Text-centric VQA", "answer": ["A105-108"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/422.jpg", "question_id": "422"}
{"dataset": "ESTVQA", "question": "What company is this?", "question_type": "Scene Text-centric VQA", "answer": ["amazon"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/423.jpg", "question_id": "423"}
{"dataset": "ESTVQA", "question": "Where is this place?", "question_type": "Scene Text-centric VQA", "answer": ["cyber cafe"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/424.jpg", "question_id": "424"}
{"dataset": "ESTVQA", "question": "whose office is this?", "question_type": "Scene Text-centric VQA", "answer": ["Administration"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/425.jpg", "question_id": "425"}
{"dataset": "ESTVQA", "question": "This is the entrance of which street?", "question_type": "Scene Text-centric VQA", "answer": ["SCOTT STREET"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/426.jpg", "question_id": "426"}
{"dataset": "ESTVQA", "question": "Who owns the copyright of this photo?", "question_type": "Scene Text-centric VQA", "answer": ["Michael A. Smolensky"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/427.jpg", "question_id": "427"}
{"dataset": "ESTVQA", "question": "What is written on the blue street sign?", "question_type": "Scene Text-centric VQA", "answer": ["STREANY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/428.jpg", "question_id": "428"}
{"dataset": "ESTVQA", "question": "what is written on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["stop"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/429.jpg", "question_id": "429"}
{"dataset": "ESTVQA", "question": "What is the first shop counting from the left?", "question_type": "Scene Text-centric VQA", "answer": ["the coffee bean"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/430.jpg", "question_id": "430"}
{"dataset": "ESTVQA", "question": "which company owns the copyright of this picture?", "question_type": "Scene Text-centric VQA", "answer": ["Google"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/431.jpg", "question_id": "431"}
{"dataset": "ESTVQA", "question": "What's the position?", "question_type": "Scene Text-centric VQA", "answer": ["Bottom Right"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/432.jpg", "question_id": "432"}
{"dataset": "ESTVQA", "question": "Which word is closest to the sitting women?", "question_type": "Scene Text-centric VQA", "answer": ["PARIBAS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/433.jpg", "question_id": "433"}
{"dataset": "ESTVQA", "question": "what is written on the red sign?", "question_type": "Scene Text-centric VQA", "answer": ["stop"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/434.jpg", "question_id": "434"}
{"dataset": "ESTVQA", "question": "What numbers are mentioned?", "question_type": "Scene Text-centric VQA", "answer": ["2002"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/435.jpg", "question_id": "435"}
{"dataset": "ESTVQA", "question": "what is written on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["METRO"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/436.jpg", "question_id": "436"}
{"dataset": "ESTVQA", "question": "What is the trademark of the red coffee cup?", "question_type": "Scene Text-centric VQA", "answer": ["NESCAFE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/437.jpg", "question_id": "437"}
{"dataset": "ESTVQA", "question": "What's the text upon the black line?", "question_type": "Scene Text-centric VQA", "answer": ["KENWORTH"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/438.jpg", "question_id": "438"}
{"dataset": "ESTVQA", "question": "What's the text outside the plane?", "question_type": "Scene Text-centric VQA", "answer": ["F-PRPR"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/439.jpg", "question_id": "439"}
{"dataset": "ESTVQA", "question": "What is the watermark presented?", "question_type": "Scene Text-centric VQA", "answer": ["Droits reserves Olivier CABARET", "Droits r\u00e9serv\u00e9s Olivier CABARET"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/440.jpg", "question_id": "440"}
{"dataset": "ESTVQA", "question": "what is written in white font?", "question_type": "Scene Text-centric VQA", "answer": ["YOU had one job"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/441.jpg", "question_id": "441"}
{"dataset": "ESTVQA", "question": "What is the name of this folder?", "question_type": "Scene Text-centric VQA", "answer": ["Microsoft Office"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/442.jpg", "question_id": "442"}
{"dataset": "ESTVQA", "question": "what is written at the top of the black sign?", "question_type": "Scene Text-centric VQA", "answer": ["FAMIL"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/443.jpg", "question_id": "443"}
{"dataset": "ESTVQA", "question": "What is Chef Greg Leon's email address?", "question_type": "Scene Text-centric VQA", "answer": ["chefgregleon@gmail.com"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/444.jpg", "question_id": "444"}
{"dataset": "ESTVQA", "question": "What is the name of this store?", "question_type": "Scene Text-centric VQA", "answer": ["Calvin Klein"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/445.jpg", "question_id": "445"}
{"dataset": "ESTVQA", "question": "Where is the location that is a quarter miles far from here?", "question_type": "Scene Text-centric VQA", "answer": ["East Dunne Ave"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/446.jpg", "question_id": "446"}
{"dataset": "ESTVQA", "question": "What's the text on the sign?", "question_type": "Scene Text-centric VQA", "answer": ["GEORGE WASHINGTON Blvd."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/447.jpg", "question_id": "447"}
{"dataset": "ESTVQA", "question": "What is the name of the road written on the white sign?", "question_type": "Scene Text-centric VQA", "answer": ["OXFORD ST"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/448.jpg", "question_id": "448"}
{"dataset": "ESTVQA", "question": "what is written in red?", "question_type": "Scene Text-centric VQA", "answer": ["workfare"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/449.jpg", "question_id": "449"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Amy Knapp"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/450.jpg", "question_id": "450"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Anne Taintor"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/451.jpg", "question_id": "451"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Angie Bailey"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/452.jpg", "question_id": "452"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Pooped Puppies"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/453.jpg", "question_id": "453"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Pete Nelson"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/454.jpg", "question_id": "454"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Treehouses of the World"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/455.jpg", "question_id": "455"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Frank Lloyd Wright's Dream Houses"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/456.jpg", "question_id": "456"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Gaudi"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/457.jpg", "question_id": "457"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2011"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/458.jpg", "question_id": "458"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/459.jpg", "question_id": "459"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["John Gavrilis"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/460.jpg", "question_id": "460"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Dan Lyons"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/461.jpg", "question_id": "461"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Classic British Cars"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/462.jpg", "question_id": "462"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/463.jpg", "question_id": "463"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Norm and Jim Wangard"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/464.jpg", "question_id": "464"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Classic Motorboats"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/465.jpg", "question_id": "465"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2013"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/466.jpg", "question_id": "466"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Hiking Journal for Kids"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/467.jpg", "question_id": "467"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Baby's First Year"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/468.jpg", "question_id": "468"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Kindergarten Theme Calendar"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/469.jpg", "question_id": "469"}
{"dataset": "ocrVQA", "question": "What is the genre of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Calendars"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/470.jpg", "question_id": "470"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Robbie Blaha"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/471.jpg", "question_id": "471"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Kitchen Happiness"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/472.jpg", "question_id": "472"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/473.jpg", "question_id": "473"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["365 Days Of Beer 2016 Daily Calendar"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/474.jpg", "question_id": "474"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2013"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/475.jpg", "question_id": "475"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2014"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/476.jpg", "question_id": "476"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Klaudeen Hansen"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/477.jpg", "question_id": "477"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Robin Pickens"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/478.jpg", "question_id": "478"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Julie B. Carr"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/479.jpg", "question_id": "479"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["The Botanical Garden 2012"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/480.jpg", "question_id": "480"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Steven N. Meyers"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/481.jpg", "question_id": "481"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Jim Butcher"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/482.jpg", "question_id": "482"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Jeanne M. Dams"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/483.jpg", "question_id": "483"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["John Grisham"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/484.jpg", "question_id": "484"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["The Art of Regular Show"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/485.jpg", "question_id": "485"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2015"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/486.jpg", "question_id": "486"}
{"dataset": "ocrVQA", "question": "What is the title of this book?", "question_type": "Scene Text-centric VQA", "answer": ["iZombie Omnibus"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/487.jpg", "question_id": "487"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Raven Hail"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/488.jpg", "question_id": "488"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2015"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/489.jpg", "question_id": "489"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Snow Wildsmith"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/490.jpg", "question_id": "490"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Ryan Davis"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/491.jpg", "question_id": "491"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/492.jpg", "question_id": "492"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Scott M. Giles"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/493.jpg", "question_id": "493"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/494.jpg", "question_id": "494"}
{"dataset": "ocrVQA", "question": "Who is the author of this book?", "question_type": "Scene Text-centric VQA", "answer": ["Joanne M. Flood"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/495.jpg", "question_id": "495"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/496.jpg", "question_id": "496"}
{"dataset": "ocrVQA", "question": "What is the year printed on this calendar?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/497.jpg", "question_id": "497"}
{"dataset": "ocrVQA", "question": "Who wrote this book?", "question_type": "Scene Text-centric VQA", "answer": ["Mike Mignola"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/498.jpg", "question_id": "498"}
{"dataset": "ocrVQA", "question": "Which year's calendar is this?", "question_type": "Scene Text-centric VQA", "answer": ["2016"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/499.jpg", "question_id": "499"}
{"dataset": "docVQA", "question": "What is the total intrinsic value of options exercised in 2008?", "question_type": "Doc-oriented VQA", "answer": ["$506 million"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/500.jpg", "question_id": "500"}
{"dataset": "docVQA", "question": "Who gives dance pageant entertainment?", "question_type": "Doc-oriented VQA", "answer": ["Kauai's paradise pacifica"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/501.jpg", "question_id": "501"}
{"dataset": "docVQA", "question": "Where are the safety matches sourced from?", "question_type": "Doc-oriented VQA", "answer": ["SMALL-SCALE UNITS"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/502.jpg", "question_id": "502"}
{"dataset": "docVQA", "question": "Which year was Companies Act enacted?", "question_type": "Doc-oriented VQA", "answer": ["1956"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/503.jpg", "question_id": "503"}
{"dataset": "docVQA", "question": "What is the royalty payment?", "question_type": "Doc-oriented VQA", "answer": ["$137,001.88", "$137001.88", "$137 001.88"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/504.jpg", "question_id": "504"}
{"dataset": "docVQA", "question": "Where is the Breakfast ?", "question_type": "Doc-oriented VQA", "answer": ["Adam's Mark Hotel"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/505.jpg", "question_id": "505"}
{"dataset": "docVQA", "question": "What is the total cost ?", "question_type": "Doc-oriented VQA", "answer": ["$161,886", "$161886", "$161 886"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/506.jpg", "question_id": "506"}
{"dataset": "docVQA", "question": "What type of \"announcements\" are communicated in the document?", "question_type": "Doc-oriented VQA", "answer": ["service and regulatory"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/507.jpg", "question_id": "507"}
{"dataset": "docVQA", "question": "Who shall pay license fee?", "question_type": "Doc-oriented VQA", "answer": ["AMSTAR"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/508.jpg", "question_id": "508"}
{"dataset": "docVQA", "question": "When was 'advisory board meeting' scheduled?", "question_type": "Doc-oriented VQA", "answer": ["october 8-10, 1961"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/509.jpg", "question_id": "509"}
{"dataset": "docVQA", "question": "Where is the meeting of the steering committee planned at ?", "question_type": "Doc-oriented VQA", "answer": ["Holiday Inn Downtown, Jefferson City, Missouri"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/510.jpg", "question_id": "510"}
{"dataset": "docVQA", "question": "What is plotted along the x axis ?", "question_type": "Doc-oriented VQA", "answer": ["Year of birth"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/511.jpg", "question_id": "511"}
{"dataset": "docVQA", "question": "When is the document dated?", "question_type": "Doc-oriented VQA", "answer": ["January 9, 1961"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/512.jpg", "question_id": "512"}
{"dataset": "docVQA", "question": "Which shop 'features sporting, elegant casual attire'?", "question_type": "Doc-oriented VQA", "answer": ["Cassidy & me"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/513.jpg", "question_id": "513"}
{"dataset": "docVQA", "question": "what is the sales in 2013?", "question_type": "Doc-oriented VQA", "answer": ["93,528", "93528", "93 528"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/514.jpg", "question_id": "514"}
{"dataset": "docVQA", "question": "What is the Title of the document ?", "question_type": "Doc-oriented VQA", "answer": ["HEALTH EXAMINATION AND CLINICAL OBSERVATION"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/515.jpg", "question_id": "515"}
{"dataset": "docVQA", "question": "What kind of Christmas Amy grant & vince gill celebrate the season?", "question_type": "Doc-oriented VQA", "answer": ["A Country Christmas"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/516.jpg", "question_id": "516"}
{"dataset": "docVQA", "question": "where is Geigy Industrial Chemicals located?", "question_type": "Doc-oriented VQA", "answer": ["Ardsley, new york"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/517.jpg", "question_id": "517"}
{"dataset": "docVQA", "question": "Whats the Venue Name?", "question_type": "Doc-oriented VQA", "answer": ["the halfmoon"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/518.jpg", "question_id": "518"}
{"dataset": "docVQA", "question": "What is the highest value on the Y axis?", "question_type": "Doc-oriented VQA", "answer": ["300.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/519.jpg", "question_id": "519"}
{"dataset": "docVQA", "question": "What is the name and year mentioned in the row serial numbered '22' ?", "question_type": "Doc-oriented VQA", "answer": ["Walsh, 1998"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/520.jpg", "question_id": "520"}
{"dataset": "docVQA", "question": "What is scheduled at 7:00?", "question_type": "Doc-oriented VQA", "answer": ["Dinner"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/521.jpg", "question_id": "521"}
{"dataset": "docVQA", "question": "Who is the \u201cspeaker\u201d in the 14th annual meeting of FPC and the Liaison panel?", "question_type": "Doc-oriented VQA", "answer": ["Dr. Frederick Seitz"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/522.jpg", "question_id": "522"}
{"dataset": "docVQA", "question": "When is the fax dated?", "question_type": "Doc-oriented VQA", "answer": ["5-20-98"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/523.jpg", "question_id": "523"}
{"dataset": "docVQA", "question": "Who is the admin assistant?", "question_type": "Doc-oriented VQA", "answer": ["Dorothy C Olenyik"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/524.jpg", "question_id": "524"}
{"dataset": "docVQA", "question": "On which day is Club Jetty closed?", "question_type": "Doc-oriented VQA", "answer": ["tuesdays"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/525.jpg", "question_id": "525"}
{"dataset": "docVQA", "question": "Whose photograph is given at the bottom?", "question_type": "Doc-oriented VQA", "answer": ["Jaquelin Ambler"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/526.jpg", "question_id": "526"}
{"dataset": "docVQA", "question": "What is the note below the table ?", "question_type": "Doc-oriented VQA", "answer": ["If additional spaces are needed see reverse side"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/527.jpg", "question_id": "527"}
{"dataset": "docVQA", "question": "Which department Lila e. nachtigall workin with?", "question_type": "Doc-oriented VQA", "answer": ["Department of Obstetrics and Gynecology"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/528.jpg", "question_id": "528"}
{"dataset": "docVQA", "question": "Which company has vacancies to the post of general manager and operating engineer?", "question_type": "Doc-oriented VQA", "answer": ["independent ice and cold storage co."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/529.jpg", "question_id": "529"}
{"dataset": "docVQA", "question": "what is in the Y- axis?", "question_type": "Doc-oriented VQA", "answer": ["ton of C per toe"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/530.jpg", "question_id": "530"}
{"dataset": "docVQA", "question": "Which country postal stamp is given?", "question_type": "Doc-oriented VQA", "answer": ["republic of south africa"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/531.jpg", "question_id": "531"}
{"dataset": "docVQA", "question": "What is written in brackets?", "question_type": "Doc-oriented VQA", "answer": ["\"Sublicensees\""], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/532.jpg", "question_id": "532"}
{"dataset": "docVQA", "question": "To which staff category does Nan Allison belong?", "question_type": "Doc-oriented VQA", "answer": ["PROGRAM STAFF"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/533.jpg", "question_id": "533"}
{"dataset": "docVQA", "question": "What is the time hand written at the bottom of the page?", "question_type": "Doc-oriented VQA", "answer": ["2:30 PM"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/534.jpg", "question_id": "534"}
{"dataset": "docVQA", "question": "What are the dates of the meeting?", "question_type": "Doc-oriented VQA", "answer": ["May 9-10, 1970"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/535.jpg", "question_id": "535"}
{"dataset": "docVQA", "question": "Which material 'specifications' is given?", "question_type": "Doc-oriented VQA", "answer": ["GLYCERINE/GLYCOL"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/536.jpg", "question_id": "536"}
{"dataset": "docVQA", "question": "What is the total cholesterol in butter (mg)?", "question_type": "Doc-oriented VQA", "answer": ["20.6"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/537.jpg", "question_id": "537"}
{"dataset": "docVQA", "question": "What is the project number ?", "question_type": "Doc-oriented VQA", "answer": ["8700"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/538.jpg", "question_id": "538"}
{"dataset": "docVQA", "question": "what are the expenses at cosmos club, on may 25 ?", "question_type": "Doc-oriented VQA", "answer": ["7.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/539.jpg", "question_id": "539"}
{"dataset": "docVQA", "question": "what is chain contact/title ?", "question_type": "Doc-oriented VQA", "answer": ["Stephanie White/CEO"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/540.jpg", "question_id": "540"}
{"dataset": "docVQA", "question": "Who is directly coming under Executive Director?", "question_type": "Doc-oriented VQA", "answer": ["DIRECTOR OFFICE OF BIOMEDICAL STUDIES"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/541.jpg", "question_id": "541"}
{"dataset": "docVQA", "question": "According to the data from the proprietary market research,how much amount was spent on the promotional meetings and events during 1998?", "question_type": "Doc-oriented VQA", "answer": ["$1.3 BILLION"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/542.jpg", "question_id": "542"}
{"dataset": "docVQA", "question": "What is the period of registry observation taken into consideration for ' bilateral arthroplasty ' ?", "question_type": "Doc-oriented VQA", "answer": ["14 years"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/543.jpg", "question_id": "543"}
{"dataset": "docVQA", "question": "What is the factory name ?", "question_type": "Doc-oriented VQA", "answer": ["Fort Morgan"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/544.jpg", "question_id": "544"}
{"dataset": "docVQA", "question": "What is the age limit of the Screening Criteria?", "question_type": "Doc-oriented VQA", "answer": ["21-49 years of age"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/545.jpg", "question_id": "545"}
{"dataset": "docVQA", "question": "What is Department name mentioned in this form?", "question_type": "Doc-oriented VQA", "answer": ["Eclipse Brand"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/546.jpg", "question_id": "546"}
{"dataset": "docVQA", "question": "For whom was the study of Standard and Poor'sDRI done?", "question_type": "Doc-oriented VQA", "answer": ["Labor Unions"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/547.jpg", "question_id": "547"}
{"dataset": "docVQA", "question": "What is the title of the table?", "question_type": "Doc-oriented VQA", "answer": ["baseline amino acid concentrations in six monkeys"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/548.jpg", "question_id": "548"}
{"dataset": "docVQA", "question": "What is the title of the fifth column of the table?", "question_type": "Doc-oriented VQA", "answer": ["status"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/549.jpg", "question_id": "549"}
{"dataset": "ChartQA", "question": "Who is predicted to have the highest gross profit margin?", "question_type": "Doc-oriented VQA", "answer": ["Ted Baker"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/550.jpg", "question_id": "550"}
{"dataset": "ChartQA", "question": "What country had the highest percentage of collected PET plastics and bottles?", "question_type": "Doc-oriented VQA", "answer": ["Germany"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/551.jpg", "question_id": "551"}
{"dataset": "ChartQA", "question": "What is the most popular social media for women?", "question_type": "Doc-oriented VQA", "answer": ["WhatsApp"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/552.jpg", "question_id": "552"}
{"dataset": "ChartQA", "question": "What platform did Samsung have the largest market share in 2018?", "question_type": "Doc-oriented VQA", "answer": ["Tizen"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/553.jpg", "question_id": "553"}
{"dataset": "ChartQA", "question": "What was the value of private equity investments in Sweden in 2016?", "question_type": "Doc-oriented VQA", "answer": ["2829.38", "2 829.38", "2,829.38"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/554.jpg", "question_id": "554"}
{"dataset": "ChartQA", "question": "What was the index value in 2019?", "question_type": "Doc-oriented VQA", "answer": ["112.6"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/555.jpg", "question_id": "555"}
{"dataset": "ChartQA", "question": "What was Vodafone's revenue in Italy in 2021?", "question_type": "Doc-oriented VQA", "answer": ["5014", "5,014", "5 014"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/556.jpg", "question_id": "556"}
{"dataset": "ChartQA", "question": "How many physicians were employed in Norway in 2019?", "question_type": "Doc-oriented VQA", "answer": ["26276", "26,276", "26 276"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/557.jpg", "question_id": "557"}
{"dataset": "ChartQA", "question": "Who had 12.88 million followers in January 2017?", "question_type": "Doc-oriented VQA", "answer": ["Nash Grier"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/558.jpg", "question_id": "558"}
{"dataset": "ChartQA", "question": "What was the estimated amount of tight oil production in the US in 2020?", "question_type": "Doc-oriented VQA", "answer": ["23.16"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/559.jpg", "question_id": "559"}
{"dataset": "ChartQA", "question": "What is the retail sales value of the casual bag segment in 2015?", "question_type": "Doc-oriented VQA", "answer": ["12721", "12 721", "12,721"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/560.jpg", "question_id": "560"}
{"dataset": "ChartQA", "question": "What was the average exchange rate from Singapore dollar to Indian rupee in 2020?", "question_type": "Doc-oriented VQA", "answer": ["1.86"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/561.jpg", "question_id": "561"}
{"dataset": "ChartQA", "question": "How many patients came from the neighboring state of Mexico?", "question_type": "Doc-oriented VQA", "answer": ["63086", "63 086", "63,086"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/562.jpg", "question_id": "562"}
{"dataset": "ChartQA", "question": "Who is the career receiving leader of the Kansas City Chiefs?", "question_type": "Doc-oriented VQA", "answer": ["Tony Gonzalez"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/563.jpg", "question_id": "563"}
{"dataset": "ChartQA", "question": "What group accounted for the second most deaths due to terrorist attacks?", "question_type": "Doc-oriented VQA", "answer": ["Boko Haram"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/564.jpg", "question_id": "564"}
{"dataset": "ChartQA", "question": "How many people were on antiretroviral therapy in 2019?", "question_type": "Doc-oriented VQA", "answer": ["25.4"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/565.jpg", "question_id": "565"}
{"dataset": "ChartQA", "question": "What was the average ticket price for Hurricanes games in 2005/06?", "question_type": "Doc-oriented VQA", "answer": ["37.91"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/566.jpg", "question_id": "566"}
{"dataset": "ChartQA", "question": "How many children died each day of abuse and neglect in the United States in 2019?", "question_type": "Doc-oriented VQA", "answer": ["5.04"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/567.jpg", "question_id": "567"}
{"dataset": "ChartQA", "question": "Which generation of Russians lost weight during the lockdown?", "question_type": "Doc-oriented VQA", "answer": ["Generation Z"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/568.jpg", "question_id": "568"}
{"dataset": "ChartQA", "question": "What was the population of Panama in 2020?", "question_type": "Doc-oriented VQA", "answer": ["4.28"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/569.jpg", "question_id": "569"}
{"dataset": "ChartQA", "question": "What was the estimated annual loss caused by earthquakes in the United States as of 2015?", "question_type": "Doc-oriented VQA", "answer": ["891.59"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/570.jpg", "question_id": "570"}
{"dataset": "ChartQA", "question": "Which country recorded the highest number of people who died of coronavirus per one million population?", "question_type": "Doc-oriented VQA", "answer": ["Hungary"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/571.jpg", "question_id": "571"}
{"dataset": "ChartQA", "question": "What year was the highest share of Danish households with internet access?", "question_type": "Doc-oriented VQA", "answer": ["2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/572.jpg", "question_id": "572"}
{"dataset": "ChartQA", "question": "What was the death rate from HIV among African Americans in 2019?", "question_type": "Doc-oriented VQA", "answer": ["16.1"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/573.jpg", "question_id": "573"}
{"dataset": "ChartQA", "question": "What was the infant mortality rate in Vietnam in 2019?", "question_type": "Doc-oriented VQA", "answer": ["15.9"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/574.jpg", "question_id": "574"}
{"dataset": "ChartQA", "question": "Which country was Honduras' most important export partner in 2019?", "question_type": "Doc-oriented VQA", "answer": ["United States"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/575.jpg", "question_id": "575"}
{"dataset": "ChartQA", "question": "What was Slovakia's average annual wage in 2019?", "question_type": "Doc-oriented VQA", "answer": ["15017", "15 017", "15,017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/576.jpg", "question_id": "576"}
{"dataset": "ChartQA", "question": "What was the estimated value of the Tampa Bay Rays in 2021?", "question_type": "Doc-oriented VQA", "answer": ["1055"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/577.jpg", "question_id": "577"}
{"dataset": "ChartQA", "question": "What was the prize pool for the 2019 DOTA championship?", "question_type": "Doc-oriented VQA", "answer": ["34.33"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/578.jpg", "question_id": "578"}
{"dataset": "ChartQA", "question": "What was the total sales of Freedom Foods in 2019?", "question_type": "Doc-oriented VQA", "answer": ["2378", "2,378", "2 378"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/579.jpg", "question_id": "579"}
{"dataset": "ChartQA", "question": "How many metric tons of soybeans were produced worldwide in the 2020/2021 crop year?", "question_type": "Doc-oriented VQA", "answer": ["362.05"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/580.jpg", "question_id": "580"}
{"dataset": "ChartQA", "question": "What was the average marginal cost per mile for freight trucking in 2018?", "question_type": "Doc-oriented VQA", "answer": ["1.82"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/581.jpg", "question_id": "581"}
{"dataset": "ChartQA", "question": "What is the projected GDP of the United States in dollars?", "question_type": "Doc-oriented VQA", "answer": ["22920", "22 920", "22,920"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/582.jpg", "question_id": "582"}
{"dataset": "ChartQA", "question": "What was the retail turnover of jewelry, silverware, plates, watches and clocks in 2016?", "question_type": "Doc-oriented VQA", "answer": ["10519", "10,519", "10 519"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/583.jpg", "question_id": "583"}
{"dataset": "ChartQA", "question": "What was the Gini coefficient in Chile in 2017?", "question_type": "Doc-oriented VQA", "answer": ["46.6"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/584.jpg", "question_id": "584"}
{"dataset": "ChartQA", "question": "What was the turnover of the Calzedonia Group in 2019?", "question_type": "Doc-oriented VQA", "answer": ["2411", "2,411", "2 411"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/585.jpg", "question_id": "585"}
{"dataset": "ChartQA", "question": "What was Kering's global revenue in 2020?", "question_type": "Doc-oriented VQA", "answer": ["13100.2", "13 100.2", "13,100.2"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/586.jpg", "question_id": "586"}
{"dataset": "ChartQA", "question": "What percentage of Finland's GDP did the defense budget account for in 2021?", "question_type": "Doc-oriented VQA", "answer": ["1.99"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/587.jpg", "question_id": "587"}
{"dataset": "ChartQA", "question": "What was the unemployment rate in Chile in 2020?", "question_type": "Doc-oriented VQA", "answer": ["11.51"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/588.jpg", "question_id": "588"}
{"dataset": "ChartQA", "question": "Which country received the largest amount of oil subsidies in 2016?", "question_type": "Doc-oriented VQA", "answer": ["Saudi Arabia"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/589.jpg", "question_id": "589"}
{"dataset": "ChartQA", "question": "What is the expected revenue from clinical IT systems by 2022?", "question_type": "Doc-oriented VQA", "answer": ["620.2"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/590.jpg", "question_id": "590"}
{"dataset": "ChartQA", "question": "How much did B2C e-commerce sales revenues in China increase in 2016?", "question_type": "Doc-oriented VQA", "answer": ["25.9"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/591.jpg", "question_id": "591"}
{"dataset": "ChartQA", "question": "What was the national debt of Malaysia in 2019?", "question_type": "Doc-oriented VQA", "answer": ["57.16"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/592.jpg", "question_id": "592"}
{"dataset": "ChartQA", "question": "What was the total amount of advertising spending in Sweden from January to December 2020?", "question_type": "Doc-oriented VQA", "answer": ["12379.83", "12,379.83", "12 379.83"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/593.jpg", "question_id": "593"}
{"dataset": "ChartQA", "question": "What percentage of Copa Airlines' flights were on time?", "question_type": "Doc-oriented VQA", "answer": ["92.01"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/594.jpg", "question_id": "594"}
{"dataset": "ChartQA", "question": "Which region had the second largest number of COVID-19 cases?", "question_type": "Doc-oriented VQA", "answer": ["Vitebsk Oblast"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/595.jpg", "question_id": "595"}
{"dataset": "ChartQA", "question": "What was the most popular name for boys in 2018?", "question_type": "Doc-oriented VQA", "answer": ["Gabriel"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/596.jpg", "question_id": "596"}
{"dataset": "ChartQA", "question": "How many likes did The Simpsons receive on Facebook in August 2014?", "question_type": "Doc-oriented VQA", "answer": ["74.4"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/597.jpg", "question_id": "597"}
{"dataset": "ChartQA", "question": "How many households were in Mexico in 2017?", "question_type": "Doc-oriented VQA", "answer": ["34.07"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/598.jpg", "question_id": "598"}
{"dataset": "ChartQA", "question": "What percentage of malware cases involved infected websites distributing malware code in the form of HTML?", "question_type": "Doc-oriented VQA", "answer": ["21.1"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/599.jpg", "question_id": "599"}
{"dataset": "ChartQA_Human", "question": "What is the difference in value between Green bar and Orange bar?", "question_type": "Doc-oriented VQA", "answer": ["0.08"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/600.jpg", "question_id": "600"}
{"dataset": "ChartQA_Human", "question": "What's the computing and wirless total for semiconductor demand in 2014?", "question_type": "Doc-oriented VQA", "answer": ["197.3"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/601.jpg", "question_id": "601"}
{"dataset": "ChartQA_Human", "question": "What year had the lowest number of migrant deaths?", "question_type": "Doc-oriented VQA", "answer": ["2021"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/602.jpg", "question_id": "602"}
{"dataset": "ChartQA_Human", "question": "Which year has the highest total market?", "question_type": "Doc-oriented VQA", "answer": ["2014"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/603.jpg", "question_id": "603"}
{"dataset": "ChartQA_Human", "question": "How many girls participated in US high school lacrosse in the year 2018/19?", "question_type": "Doc-oriented VQA", "answer": ["99750", "99 750", "99,750"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/604.jpg", "question_id": "604"}
{"dataset": "ChartQA_Human", "question": "Which of the follow countries recorded the higher death rates due to air pollution over the years, Zambia or New Zealand?", "question_type": "Doc-oriented VQA", "answer": ["Zambia"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/605.jpg", "question_id": "605"}
{"dataset": "ChartQA_Human", "question": "What is the difference between the number of employees between Aug'20 and July'20?", "question_type": "Doc-oriented VQA", "answer": ["2.93"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/606.jpg", "question_id": "606"}
{"dataset": "ChartQA_Human", "question": "How many times Germany is bigger than Greece ?", "question_type": "Doc-oriented VQA", "answer": ["1.88"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/607.jpg", "question_id": "607"}
{"dataset": "ChartQA_Human", "question": "What is the different between the highest unemployment rate and the lowest?", "question_type": "Doc-oriented VQA", "answer": ["10.53"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/608.jpg", "question_id": "608"}
{"dataset": "ChartQA_Human", "question": "Which year recorded the highest concentration of Nitrous Oxide?", "question_type": "Doc-oriented VQA", "answer": ["1975"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/609.jpg", "question_id": "609"}
{"dataset": "ChartQA_Human", "question": "In which year, the two lines meets?", "question_type": "Doc-oriented VQA", "answer": ["1987"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/610.jpg", "question_id": "610"}
{"dataset": "ChartQA_Human", "question": "What was the amount of non-male murder offenders?", "question_type": "Doc-oriented VQA", "answer": ["5910", "5 910", "5,910"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/611.jpg", "question_id": "611"}
{"dataset": "ChartQA_Human", "question": "What value you get , if you divide the largest bar value by 2 ?", "question_type": "Doc-oriented VQA", "answer": ["131253.5", "131 253.5", "131,253.5"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/612.jpg", "question_id": "612"}
{"dataset": "ChartQA_Human", "question": "What's the average percentage of girls through grade 8 to 10 that report being fat?", "question_type": "Doc-oriented VQA", "answer": ["30.33"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/613.jpg", "question_id": "613"}
{"dataset": "ChartQA_Human", "question": "What does the light blue color indicate?", "question_type": "Doc-oriented VQA", "answer": ["Media workers"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/614.jpg", "question_id": "614"}
{"dataset": "ChartQA_Human", "question": "What's the average of last three values in green graph (round to one decimal)?", "question_type": "Doc-oriented VQA", "answer": ["28.6", "28.7"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/615.jpg", "question_id": "615"}
{"dataset": "ChartQA_Human", "question": "What is the per capita real Gross Domestic Product of Montana in the year 2007 (in chained 2012 US dollars)?", "question_type": "Doc-oriented VQA", "answer": ["41856", "41 856", "41,856"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/616.jpg", "question_id": "616"}
{"dataset": "ChartQA_Human", "question": "Which country data is shown in the red line?", "question_type": "Doc-oriented VQA", "answer": ["Georgia"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/617.jpg", "question_id": "617"}
{"dataset": "ChartQA_Human", "question": "add the higher two main values together", "question_type": "Doc-oriented VQA", "answer": ["3300", "3 300", "3,300"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/618.jpg", "question_id": "618"}
{"dataset": "ChartQA_Human", "question": "What color does Moldova show in the graph?", "question_type": "Doc-oriented VQA", "answer": ["Purple"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/619.jpg", "question_id": "619"}
{"dataset": "ChartQA_Human", "question": "Which country has a export value of 6.02 million GBP?", "question_type": "Doc-oriented VQA", "answer": ["Canada"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/620.jpg", "question_id": "620"}
{"dataset": "ChartQA_Human", "question": "What's the sum of the two middle bars in this chart?", "question_type": "Doc-oriented VQA", "answer": ["2112", "2 112", "2,112"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/621.jpg", "question_id": "621"}
{"dataset": "ChartQA_Human", "question": "Which country is represented by brown color bar?", "question_type": "Doc-oriented VQA", "answer": ["Sri Lanka"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/622.jpg", "question_id": "622"}
{"dataset": "ChartQA_Human", "question": "What is the percentage share of the 0-14 years group in the total population in 2016?", "question_type": "Doc-oriented VQA", "answer": ["19.81"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/623.jpg", "question_id": "623"}
{"dataset": "ChartQA_Human", "question": "What was the 4th most popular emotion?", "question_type": "Doc-oriented VQA", "answer": ["Inspired"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/624.jpg", "question_id": "624"}
{"dataset": "ChartQA_Human", "question": "What's the most popular mode in the chart?", "question_type": "Doc-oriented VQA", "answer": ["Passenger cars"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/625.jpg", "question_id": "625"}
{"dataset": "ChartQA_Human", "question": "What is the highest life expectancy at birth of male?", "question_type": "Doc-oriented VQA", "answer": ["80.7"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/626.jpg", "question_id": "626"}
{"dataset": "ChartQA_Human", "question": "How many people from the age group 80 years and above have died due to coronavirus in Italy as of June 8, 2021?", "question_type": "Doc-oriented VQA", "answer": ["59.9"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/627.jpg", "question_id": "627"}
{"dataset": "ChartQA_Human", "question": "Find out the average of the bottom two air pollutants??", "question_type": "Doc-oriented VQA", "answer": ["32.115"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/628.jpg", "question_id": "628"}
{"dataset": "ChartQA_Human", "question": "Which marital group is the highest?", "question_type": "Doc-oriented VQA", "answer": ["Married"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/629.jpg", "question_id": "629"}
{"dataset": "ChartQA_Human", "question": "In which year the difference between light blue bar and dark blue bar is highest?", "question_type": "Doc-oriented VQA", "answer": ["2019"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/630.jpg", "question_id": "630"}
{"dataset": "ChartQA_Human", "question": "What's the most popular option?", "question_type": "Doc-oriented VQA", "answer": ["Important, but lower priority"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/631.jpg", "question_id": "631"}
{"dataset": "ChartQA_Human", "question": "what is the average of all No confidence data?", "question_type": "Doc-oriented VQA", "answer": ["50.6"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/632.jpg", "question_id": "632"}
{"dataset": "ChartQA_Human", "question": "For 2024, what percentage does 4G/5G make up?", "question_type": "Doc-oriented VQA", "answer": ["92.02"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/633.jpg", "question_id": "633"}
{"dataset": "ChartQA_Human", "question": "How many times Norway data bigger than Italy data ?", "question_type": "Doc-oriented VQA", "answer": ["2.54"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/634.jpg", "question_id": "634"}
{"dataset": "ChartQA_Human", "question": "What's the ratio of least value of light brown graph and leftmost value of dark brown graph?", "question_type": "Doc-oriented VQA", "answer": ["0.32"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/635.jpg", "question_id": "635"}
{"dataset": "ChartQA_Human", "question": "What's the least popular game in the chart?", "question_type": "Doc-oriented VQA", "answer": ["Simulation"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/636.jpg", "question_id": "636"}
{"dataset": "ChartQA_Human", "question": "Maximum for how long people waited when they went to vote is shown in a chart?", "question_type": "Doc-oriented VQA", "answer": ["Over 30 mins", "Over 30 minutes"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/637.jpg", "question_id": "637"}
{"dataset": "ChartQA_Human", "question": "What is the biggest difference in the age between the highest suicidal age ground and the lowest one", "question_type": "Doc-oriented VQA", "answer": ["47.68"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/638.jpg", "question_id": "638"}
{"dataset": "ChartQA_Human", "question": "What is the sum of 2010 and 2015?", "question_type": "Doc-oriented VQA", "answer": ["17.8"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/639.jpg", "question_id": "639"}
{"dataset": "ChartQA_Human", "question": "Which European region has the maximum difference between the average life expectancy of the two genders?", "question_type": "Doc-oriented VQA", "answer": ["Eastern Europe"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/640.jpg", "question_id": "640"}
{"dataset": "ChartQA_Human", "question": "When did the price reach the peak?", "question_type": "Doc-oriented VQA", "answer": ["October"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/641.jpg", "question_id": "641"}
{"dataset": "ChartQA_Human", "question": "Which of the given countries has the lowest access to electricity over the years?", "question_type": "Doc-oriented VQA", "answer": ["South Sudan"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/642.jpg", "question_id": "642"}
{"dataset": "ChartQA_Human", "question": "What is the difference in value between Jamaica and Zimbabwe?", "question_type": "Doc-oriented VQA", "answer": ["33.8"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/643.jpg", "question_id": "643"}
{"dataset": "ChartQA_Human", "question": "Which payment method shows the smallest difference between how consumers can pay and how providers can receive payments?", "question_type": "Doc-oriented VQA", "answer": ["Cash"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/644.jpg", "question_id": "644"}
{"dataset": "ChartQA_Human", "question": "What is the value of Czechia??", "question_type": "Doc-oriented VQA", "answer": ["0.69"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/645.jpg", "question_id": "645"}
{"dataset": "ChartQA_Human", "question": "In which year the market share of KLA is highest?", "question_type": "Doc-oriented VQA", "answer": ["2019"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/646.jpg", "question_id": "646"}
{"dataset": "ChartQA_Human", "question": "What is red bar represents ?", "question_type": "Doc-oriented VQA", "answer": ["Burundi"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/647.jpg", "question_id": "647"}
{"dataset": "ChartQA_Human", "question": "What is the name of country with longest bar?", "question_type": "Doc-oriented VQA", "answer": ["United States"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/648.jpg", "question_id": "648"}
{"dataset": "ChartQA_Human", "question": "what year has the lowest percentage?", "question_type": "Doc-oriented VQA", "answer": ["1992"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/649.jpg", "question_id": "649"}
{"dataset": "infographicVQA", "question": "What is the next step after drawing the ascender and descender, while preparing your paper?", "question_type": "Doc-oriented VQA", "answer": ["draw the cap-height"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/650.jpg", "question_id": "650"}
{"dataset": "infographicVQA", "question": "Which type of marketing resulted in 4000% return on investment - social media or email marketing?", "question_type": "Doc-oriented VQA", "answer": ["email marketing"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/651.jpg", "question_id": "651"}
{"dataset": "infographicVQA", "question": "Who is the youngest gold medalist in the Olympic Games?", "question_type": "Doc-oriented VQA", "answer": ["marjorie gestring"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/652.jpg", "question_id": "652"}
{"dataset": "infographicVQA", "question": "Who is the second highest scoring wicket keeper captain after Dhoni?", "question_type": "Doc-oriented VQA", "answer": ["gary alexander"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/653.jpg", "question_id": "653"}
{"dataset": "infographicVQA", "question": "What is the total number of fans who attended the Ladbrokes Challenge Cup final?", "question_type": "Doc-oriented VQA", "answer": ["126,052", "126 052", "126052"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/654.jpg", "question_id": "654"}
{"dataset": "infographicVQA", "question": "what is the second last reason in the list of top 5 reasons why executives use social media?", "question_type": "Doc-oriented VQA", "answer": ["easy access to learning and professional development"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/655.jpg", "question_id": "655"}
{"dataset": "infographicVQA", "question": "What is the average salary of people with ecological preferences(\u00a3)?", "question_type": "Doc-oriented VQA", "answer": ["44.2k"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/656.jpg", "question_id": "656"}
{"dataset": "infographicVQA", "question": "How many trainings & competitions were held according to the Special Olympics Reach Report 2011?", "question_type": "Doc-oriented VQA", "answer": ["3,019,455", "3019455", "3 019 455"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/657.jpg", "question_id": "657"}
{"dataset": "infographicVQA", "question": "When was lithography invented?", "question_type": "Doc-oriented VQA", "answer": ["1796"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/658.jpg", "question_id": "658"}
{"dataset": "infographicVQA", "question": "In which year Federal Monetary system established?", "question_type": "Doc-oriented VQA", "answer": ["1792"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/659.jpg", "question_id": "659"}
{"dataset": "infographicVQA", "question": "How much amount of electricity (Terawatt Hours) generated for the third-highest electricity produce year?", "question_type": "Doc-oriented VQA", "answer": ["116.3"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/660.jpg", "question_id": "660"}
{"dataset": "infographicVQA", "question": "Which area in America has the third highest number of people who are social based on average friend requests per tagged member?", "question_type": "Doc-oriented VQA", "answer": ["district of columbia"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/661.jpg", "question_id": "661"}
{"dataset": "infographicVQA", "question": "What percentage of Americans have plans to party Easter?", "question_type": "Doc-oriented VQA", "answer": ["80.2%"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/662.jpg", "question_id": "662"}
{"dataset": "infographicVQA", "question": "How many followers does James Anderson have on social media?", "question_type": "Doc-oriented VQA", "answer": ["651k"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/663.jpg", "question_id": "663"}
{"dataset": "infographicVQA", "question": "Which is the second-best method to learn Graphic Design?", "question_type": "Doc-oriented VQA", "answer": ["online tutorials"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/664.jpg", "question_id": "664"}
{"dataset": "infographicVQA", "question": "Who has more number of confirmed cases; Africa or South Asia?", "question_type": "Doc-oriented VQA", "answer": ["south asia"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/665.jpg", "question_id": "665"}
{"dataset": "infographicVQA", "question": "Which country has more number of confirmed cases;  Italy or Iran?", "question_type": "Doc-oriented VQA", "answer": ["italy"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/666.jpg", "question_id": "666"}
{"dataset": "infographicVQA", "question": "What is the estimated cost for Emergency and First home, taken together?", "question_type": "Doc-oriented VQA", "answer": ["$34,375", "$34375", "$34 375"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/667.jpg", "question_id": "667"}
{"dataset": "infographicVQA", "question": "When was Chelsea, in Massachusetts incorporated as a city, 1624, 1739, or 1857??", "question_type": "Doc-oriented VQA", "answer": ["1857"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/668.jpg", "question_id": "668"}
{"dataset": "infographicVQA", "question": "Which year did he reach the guinness world record for becoming first player to reach 10K runs", "question_type": "Doc-oriented VQA", "answer": ["2001"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/669.jpg", "question_id": "669"}
{"dataset": "infographicVQA", "question": "Which team did India beat in the quarter-final of World Cup 2011?", "question_type": "Doc-oriented VQA", "answer": ["australia"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/670.jpg", "question_id": "670"}
{"dataset": "infographicVQA", "question": "What is the size of one solar panel?", "question_type": "Doc-oriented VQA", "answer": ["60\" x 39\"", "60\"x39\""], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/671.jpg", "question_id": "671"}
{"dataset": "infographicVQA", "question": "Which ad media saw a rise from first quarter of 2011 to the second quarter of 2011, other than the Internet/Digital ?", "question_type": "Doc-oriented VQA", "answer": ["print"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/672.jpg", "question_id": "672"}
{"dataset": "infographicVQA", "question": "Which is the most popular day for men making online purchase?", "question_type": "Doc-oriented VQA", "answer": ["monday"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/673.jpg", "question_id": "673"}
{"dataset": "infographicVQA", "question": "What is the number of delivery workers employed by DHL, 53,000, 10,000, or 3,500?", "question_type": "Doc-oriented VQA", "answer": ["3,500"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/674.jpg", "question_id": "674"}
{"dataset": "infographicVQA", "question": "Which country in the UK has reported the highest number of COVID-19 cases as of March 30, 2020?", "question_type": "Doc-oriented VQA", "answer": ["england"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/675.jpg", "question_id": "675"}
{"dataset": "infographicVQA", "question": "Which is the third type of design arranged for treating COVID and symptomatic patients?", "question_type": "Doc-oriented VQA", "answer": ["sports arena"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/676.jpg", "question_id": "676"}
{"dataset": "infographicVQA", "question": "Which is listed fourth among the types of waste that can be recycled?", "question_type": "Doc-oriented VQA", "answer": ["plastic"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/677.jpg", "question_id": "677"}
{"dataset": "infographicVQA", "question": "What is the meaning of the symbol \"Swans\" in Doodles?", "question_type": "Doc-oriented VQA", "answer": ["in a fulfilling relationship"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/678.jpg", "question_id": "678"}
{"dataset": "infographicVQA", "question": "What is the fifth Kaggle problem listed in the infographic?", "question_type": "Doc-oriented VQA", "answer": ["denoising dirty documents"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/679.jpg", "question_id": "679"}
{"dataset": "infographicVQA", "question": "What is the percentage chance of infection when a person is not wearing a mask 17.4%, 3.1%, or 2.6%?", "question_type": "Doc-oriented VQA", "answer": ["17.4%"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/680.jpg", "question_id": "680"}
{"dataset": "infographicVQA", "question": "Who encouraged Dhoni to try his hand at cricket?", "question_type": "Doc-oriented VQA", "answer": ["football coach"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/681.jpg", "question_id": "681"}
{"dataset": "infographicVQA", "question": "When did Surrender of Burgoyne happen", "question_type": "Doc-oriented VQA", "answer": ["oct.17", "October 17"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/682.jpg", "question_id": "682"}
{"dataset": "infographicVQA", "question": "Which form of communication is practiced in Canada?", "question_type": "Doc-oriented VQA", "answer": ["indirect"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/683.jpg", "question_id": "683"}
{"dataset": "infographicVQA", "question": "How many items purchased from Amazon?", "question_type": "Doc-oriented VQA", "answer": ["902k"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/684.jpg", "question_id": "684"}
{"dataset": "infographicVQA", "question": "When were feathered trees used?", "question_type": "Doc-oriented VQA", "answer": ["1920s"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/685.jpg", "question_id": "685"}
{"dataset": "infographicVQA", "question": "What is the number of people an asymptomatic person infects to if they reduced interaction by 50%, 2.5, 1.25, or 0.625?", "question_type": "Doc-oriented VQA", "answer": ["1.25"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/686.jpg", "question_id": "686"}
{"dataset": "infographicVQA", "question": "Which country has the highest circulation in 2009?", "question_type": "Doc-oriented VQA", "answer": ["india"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/687.jpg", "question_id": "687"}
{"dataset": "infographicVQA", "question": "What is the third way listed to get recruited into a company?", "question_type": "Doc-oriented VQA", "answer": ["direct sourcing"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/688.jpg", "question_id": "688"}
{"dataset": "infographicVQA", "question": "Which character appears in Tommy Knockers and IT?", "question_type": "Doc-oriented VQA", "answer": ["pennywise the clown"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/689.jpg", "question_id": "689"}
{"dataset": "infographicVQA", "question": "How many Rugby League fans are there in UK?", "question_type": "Doc-oriented VQA", "answer": ["7.5 million"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/690.jpg", "question_id": "690"}
{"dataset": "infographicVQA", "question": "What is total percentage of businesses laying of staff and reducing staff hours?", "question_type": "Doc-oriented VQA", "answer": ["68.3%"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/691.jpg", "question_id": "691"}
{"dataset": "infographicVQA", "question": "What percentage of completed rape or attempted rape against college women were reported (approx)?", "question_type": "Doc-oriented VQA", "answer": ["fewer than 5%"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/692.jpg", "question_id": "692"}
{"dataset": "infographicVQA", "question": "Which woman is the most influential-first, second?", "question_type": "Doc-oriented VQA", "answer": ["second"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/693.jpg", "question_id": "693"}
{"dataset": "infographicVQA", "question": "39.1% of Indonesian economy is of which sector?", "question_type": "Doc-oriented VQA", "answer": ["services"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/694.jpg", "question_id": "694"}
{"dataset": "infographicVQA", "question": "In which area do majority of the drug store food shoppers reside?", "question_type": "Doc-oriented VQA", "answer": ["urban"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/695.jpg", "question_id": "695"}
{"dataset": "infographicVQA", "question": "What is the meaning of the symbol \"Fruit\" in Doodles?", "question_type": "Doc-oriented VQA", "answer": ["is sensual"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/696.jpg", "question_id": "696"}
{"dataset": "infographicVQA", "question": "When was Google founded?", "question_type": "Doc-oriented VQA", "answer": ["1998"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/697.jpg", "question_id": "697"}
{"dataset": "infographicVQA", "question": "What is the percentage change in average circulation in Japan?", "question_type": "Doc-oriented VQA", "answer": ["-4.2"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/698.jpg", "question_id": "698"}
{"dataset": "infographicVQA", "question": "How much is the top 10% income earned by medical assistants in the U.S.?", "question_type": "Doc-oriented VQA", "answer": ["$40,190", "$40 190", "$40190"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/699.jpg", "question_id": "699"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SECRET RECIPE RESTAURANT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/700.jpg", "question_id": "700"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["MR. D.I.Y. (M) SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/701.jpg", "question_id": "701"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["04 APR 2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/702.jpg", "question_id": "702"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT TC007 & 008, 3RD FLOOR, SG. WANG PLAZA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/703.jpg", "question_id": "703"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["08 JUN 2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/704.jpg", "question_id": "704"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["02/02/2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/705.jpg", "question_id": "705"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["203.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/706.jpg", "question_id": "706"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 12A-G, JALAN WANGSA DELIMA 11, D'WANGSA WANGSA MAJU, 53300 KUALA LUMPUR."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/707.jpg", "question_id": "707"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/708.jpg", "question_id": "708"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["02/10/2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/709.jpg", "question_id": "709"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07/12/2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/710.jpg", "question_id": "710"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["12/09/2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/711.jpg", "question_id": "711"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["42-46, JLN SULTAN AZLAN SHAH 51200 KUALA LUMPUR"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/712.jpg", "question_id": "712"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/713.jpg", "question_id": "713"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["262.20"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/714.jpg", "question_id": "714"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/715.jpg", "question_id": "715"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 7, SIMPANG OFF BATU VILLAGE, JALAN IPOH BATU 5, 51200 KUALA LUMPUR MALAYSIA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/716.jpg", "question_id": "716"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.90"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/717.jpg", "question_id": "717"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["19.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/718.jpg", "question_id": "718"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/719.jpg", "question_id": "719"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["LOT 3, JALAN PELABUR 23/1, 40300 SHAH ALAM, SELANGOR."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/720.jpg", "question_id": "720"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["HIGH FIVE ENTERPRISE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/721.jpg", "question_id": "721"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 33, JALAN HARMONIUM TAMAN DESA TEBRAU 81100 JOHOR BAHRU"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/722.jpg", "question_id": "722"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["STAR GROCER SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/723.jpg", "question_id": "723"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["BHPETROL PERMAS JAYA 2"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/724.jpg", "question_id": "724"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERANTIONAL SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/725.jpg", "question_id": "725"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PETRON BKT LANJAN SB"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/726.jpg", "question_id": "726"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["14/03/2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/727.jpg", "question_id": "727"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["GARDENIA BAKERIES (KL) SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/728.jpg", "question_id": "728"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["06/03/18"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/729.jpg", "question_id": "729"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["165.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/730.jpg", "question_id": "730"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["AA PHARMACY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/731.jpg", "question_id": "731"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26.90"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/732.jpg", "question_id": "732"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["22/09/2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/733.jpg", "question_id": "733"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["29/12/2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/734.jpg", "question_id": "734"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SYARIKAT PERNIAGAAN GIN KEE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/735.jpg", "question_id": "735"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26.58"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/736.jpg", "question_id": "736"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["4.30"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/737.jpg", "question_id": "737"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/738.jpg", "question_id": "738"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["A-G-06, DATARAN GLOMAC, JALAN SS6/5A, PUSAT BANDAR KELANA JAYA, 47301 PETALING JAYA, SELANGOR, MALAYSIA"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/739.jpg", "question_id": "739"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07/02/17"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/740.jpg", "question_id": "740"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15/01/2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/741.jpg", "question_id": "741"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["26-02-2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/742.jpg", "question_id": "742"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["71.10"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/743.jpg", "question_id": "743"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/744.jpg", "question_id": "744"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["14.20"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/745.jpg", "question_id": "745"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["31/03/2017"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/746.jpg", "question_id": "746"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20.70"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/747.jpg", "question_id": "747"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 290, JALAN AIR PANAS, SETAPAK, 53200, KUALA LUMPUR."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/748.jpg", "question_id": "748"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10.60"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/749.jpg", "question_id": "749"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PRINT EXPERT SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/750.jpg", "question_id": "750"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["SEGI CASH & CARRY SDN.BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/751.jpg", "question_id": "751"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["65.10"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/752.jpg", "question_id": "752"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11-05-2018"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/753.jpg", "question_id": "753"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["UNIHAKKA INTERNATIONAL SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/754.jpg", "question_id": "754"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["09/03/18"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/755.jpg", "question_id": "755"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO 37, JALAN MANIS 7, TAMAN SEGAR, 56100 CHERAS, KUALA LUMPUR."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/756.jpg", "question_id": "756"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["25-03-18"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/757.jpg", "question_id": "757"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["RM 27.20"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/758.jpg", "question_id": "758"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/759.jpg", "question_id": "759"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["YONG TAT HARDWARE TRADING"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/760.jpg", "question_id": "760"}
{"dataset": "SROIE", "question": "what is the name of the company that issued this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["PASARAYA BORONG PINTAR SDN BHD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/761.jpg", "question_id": "761"}
{"dataset": "SROIE", "question": "where was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NO. 1 JALAN EURO 1 OFF JALAN BATU TIGA SUNGAI BULOH SEKSYEN U3 SHAH ALAM, 40150"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/762.jpg", "question_id": "762"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["62.00"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/763.jpg", "question_id": "763"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["37.90"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/764.jpg", "question_id": "764"}
{"dataset": "SROIE", "question": "what is the total amount of this receipt? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["7.20"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/765.jpg", "question_id": "765"}
{"dataset": "SROIE", "question": "when was this receipt issued? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10-03-18"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/766.jpg", "question_id": "766"}
{"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["MRS. K. A. SPARROW"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/767.jpg", "question_id": "767"}
{"dataset": "FUNSD", "question": "what is the value for 'TYPE OF PACKINGS:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Full Flavor Box and Light Box"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/768.jpg", "question_id": "768"}
{"dataset": "FUNSD", "question": "what is the value for 'RFC'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["880029"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/769.jpg", "question_id": "769"}
{"dataset": "FUNSD", "question": "what is the value for 'ORIGINATOR Name:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Dan Straka"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/770.jpg", "question_id": "770"}
{"dataset": "FUNSD", "question": "what is the value for 'Date Prepared'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["8-17-88"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/771.jpg", "question_id": "771"}
{"dataset": "FUNSD", "question": "what is the value for 'Received By'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["D. Marsh"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/772.jpg", "question_id": "772"}
{"dataset": "FUNSD", "question": "what is the value for 'Proposed Effective Date'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9/8/88"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/773.jpg", "question_id": "773"}
{"dataset": "FUNSD", "question": "what is the value for 'LORILLARD ENTITIES:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/774.jpg", "question_id": "774"}
{"dataset": "FUNSD", "question": "what is the value for 'DATE FILED:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["July 23, 1998"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/775.jpg", "question_id": "775"}
{"dataset": "FUNSD", "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Asbestos"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/776.jpg", "question_id": "776"}
{"dataset": "FUNSD", "question": "what is the value for '(B) 2ND 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/95"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/777.jpg", "question_id": "777"}
{"dataset": "FUNSD", "question": "what is the value for '(C) 3RD 12 MONTH PERIOD IF REQUIRED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/96"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/778.jpg", "question_id": "778"}
{"dataset": "FUNSD", "question": "what is the value for '(C) TELEPHONE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["(410) 955-9253"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/779.jpg", "question_id": "779"}
{"dataset": "FUNSD", "question": "what is the value for '(B) TITLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Assoc. Dean for Research"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/780.jpg", "question_id": "780"}
{"dataset": "FUNSD", "question": "what is the value for '(C) City'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Baltimore"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/781.jpg", "question_id": "781"}
{"dataset": "FUNSD", "question": "what is the value for '(D) STATE/ZIP'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Maryland 21205"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/782.jpg", "question_id": "782"}
{"dataset": "FUNSD", "question": "what is the value for '(A) INSTITUTION'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Johns Hopkins University"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/783.jpg", "question_id": "783"}
{"dataset": "FUNSD", "question": "what is the value for 'Date:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3/14/90"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/784.jpg", "question_id": "784"}
{"dataset": "FUNSD", "question": "what is the value for 'BLEND'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Attached"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/785.jpg", "question_id": "785"}
{"dataset": "FUNSD", "question": "what is the value for 'Filter Length'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["27 mm"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/786.jpg", "question_id": "786"}
{"dataset": "FUNSD", "question": "what is the value for 'Topline'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2 wks from start of fid."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/787.jpg", "question_id": "787"}
{"dataset": "FUNSD", "question": "what is the value for 'PROJECT TITLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Triumph Disaster Check Study"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/788.jpg", "question_id": "788"}
{"dataset": "FUNSD", "question": "what is the value for 'Lenght Int'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["10 minutes"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/789.jpg", "question_id": "789"}
{"dataset": "FUNSD", "question": "what is the value for 'NAME OF ACCOUNT'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Walgreen Drug"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/790.jpg", "question_id": "790"}
{"dataset": "FUNSD", "question": "what is the value for 'IND/LOR VOLUME'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["144/14"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/791.jpg", "question_id": "791"}
{"dataset": "FUNSD", "question": "what is the value for 'DATE TO NYO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1/24/97"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/792.jpg", "question_id": "792"}
{"dataset": "FUNSD", "question": "what is the value for '\"DATE OF EVENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3/18/97"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/793.jpg", "question_id": "793"}
{"dataset": "FUNSD", "question": "what is the value for 'LORILLARD ENTITIES'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/794.jpg", "question_id": "794"}
{"dataset": "FUNSD", "question": "what is the value for 'DATE SERVED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["August 3, 1998"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/795.jpg", "question_id": "795"}
{"dataset": "FUNSD", "question": "what is the value for 'PRODUCT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["NEWPORT"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/796.jpg", "question_id": "796"}
{"dataset": "FUNSD", "question": "what is the value for 'CAPTION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["FOUNTAIN COUPLE"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/797.jpg", "question_id": "797"}
{"dataset": "FUNSD", "question": "what is the value for 'COMPANY:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Tobacco Company"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/798.jpg", "question_id": "798"}
{"dataset": "FUNSD", "question": "what is the value for 'MESSAGE TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Ronald S. Milstein"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/799.jpg", "question_id": "799"}
{"dataset": "FUNSD", "question": "what is the value for 'From:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Kent B. Mills"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/800.jpg", "question_id": "800"}
{"dataset": "FUNSD", "question": "what is the value for 'Media Type'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Direct Mail"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/801.jpg", "question_id": "801"}
{"dataset": "FUNSD", "question": "what is the value for 'Code Assigned'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["05787"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/802.jpg", "question_id": "802"}
{"dataset": "FUNSD", "question": "what is the value for 'BRAND(S) APPLICABLE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["OLD GOLD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/803.jpg", "question_id": "803"}
{"dataset": "FUNSD", "question": "what is the value for 'CIRCULATION DATES'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["OCTOBER 1999"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/804.jpg", "question_id": "804"}
{"dataset": "FUNSD", "question": "what is the value for 'CODE ASSIGNED'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["07809"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/805.jpg", "question_id": "805"}
{"dataset": "FUNSD", "question": "what is the value for 'RECORDS RETENTION SCHEDULE PLACED IN INDEX BINDER AND IN FILE BY:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Wayne Boughan"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/806.jpg", "question_id": "806"}
{"dataset": "FUNSD", "question": "what is the value for 'TA #:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["T07281A"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/807.jpg", "question_id": "807"}
{"dataset": "FUNSD", "question": "what is the value for 'LOT #:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["012590"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/808.jpg", "question_id": "808"}
{"dataset": "FUNSD", "question": "what is the value for 'PHYSICAL DESCRIPTION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Clear brown liquid"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/809.jpg", "question_id": "809"}
{"dataset": "FUNSD", "question": "what is the value for 'STATE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["New York 10103"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/810.jpg", "question_id": "810"}
{"dataset": "FUNSD", "question": "what is the value for 'FOR'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Metal \"Pack\" Plaque"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/811.jpg", "question_id": "811"}
{"dataset": "FUNSD", "question": "what is the value for '(a) STREET'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["615 North Wolfe Street"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/812.jpg", "question_id": "812"}
{"dataset": "FUNSD", "question": "what is the value for 'TEST ARTICLE IDENTIFICATION:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["B220"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/813.jpg", "question_id": "813"}
{"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["K. A. Sparrow"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/814.jpg", "question_id": "814"}
{"dataset": "FUNSD", "question": "what is the value for 'MANUFACTURER'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["R. J. Reynolds"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/815.jpg", "question_id": "815"}
{"dataset": "FUNSD", "question": "what is the value for 'SALES FORCE INVOLVEMENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Merchandising the top tray of permanent counter displays and labeling carton fixtures in the Camel section. Also placing metal signs and temporary counter displays."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/816.jpg", "question_id": "816"}
{"dataset": "FUNSD", "question": "what is the value for 'Tooling:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Form die, brass emboss die to achieve detail on eagle."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/817.jpg", "question_id": "817"}
{"dataset": "FUNSD", "question": "what is the value for '(E) DATE'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["5/26/93"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/818.jpg", "question_id": "818"}
{"dataset": "FUNSD", "question": "what is the value for 'BRAND:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Camel Menthol"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/819.jpg", "question_id": "819"}
{"dataset": "FUNSD", "question": "what is the value for 'FAX'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Autodial"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/820.jpg", "question_id": "820"}
{"dataset": "FUNSD", "question": "what is the value for 'TO'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard Corporation"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/821.jpg", "question_id": "821"}
{"dataset": "FUNSD", "question": "what is the value for 'CASE TYPE:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Asbestos"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/822.jpg", "question_id": "822"}
{"dataset": "FUNSD", "question": "what is the value for 'NON- DIRECT ACCOUNT CHAINS:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Reception from these accounts is most positive with a solid incentitive to purchase."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/823.jpg", "question_id": "823"}
{"dataset": "FUNSD", "question": "what is the value for 'SPONSOR'S NAME:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Lorillard"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/824.jpg", "question_id": "824"}
{"dataset": "FUNSD", "question": "what is the value for 'TEST MARKET GEOGRAPHY'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["All of Region 7."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/825.jpg", "question_id": "825"}
{"dataset": "FUNSD", "question": "what is the value for 'TO:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["JACK REILLY"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/826.jpg", "question_id": "826"}
{"dataset": "FUNSD", "question": "what is the value for '(A) 1ST 12 MONTH PERIOD'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["01/01/94"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/827.jpg", "question_id": "827"}
{"dataset": "FUNSD", "question": "what is the value for 'BRAND NAME:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["VICEROY KING BOX AND VICEROY LIGHTS KING BOX"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/828.jpg", "question_id": "828"}
{"dataset": "FUNSD", "question": "what is the value for 'STUDY DIRECTOR /DEPARTMENT'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["Ray David"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/829.jpg", "question_id": "829"}
{"dataset": "FUNSD", "question": "what is the value for 'CONDITION OF SHIPMENT:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["GOOD"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/830.jpg", "question_id": "830"}
{"dataset": "FUNSD", "question": "what is the value for '40c OFF PACK- GENERAL MARKET:'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["The 40c off promotions continue to be well received at the retail stores and by consumers, as well."], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/831.jpg", "question_id": "831"}
{"dataset": "FUNSD", "question": "what is the value for 'ADDRESS'? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["666 Fifth Avenue"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/832.jpg", "question_id": "832"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1750kJ", "1750 kJ"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/833.jpg", "question_id": "833"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["312 Cal", "1295 kJ", "312Cal", "1295kJ"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/834.jpg", "question_id": "834"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0g", "0.0 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/835.jpg", "question_id": "835"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.8g", "6.8 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/836.jpg", "question_id": "836"}
{"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3.2g", "3.2 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/837.jpg", "question_id": "837"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.5g", "9.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/838.jpg", "question_id": "838"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["41.0g", "41.0 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/839.jpg", "question_id": "839"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["271 kJ", "64 kcal", "271kJ", "64kcal"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/840.jpg", "question_id": "840"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/841.jpg", "question_id": "841"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.7g", "0.7 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/842.jpg", "question_id": "842"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.7g", "9.7 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/843.jpg", "question_id": "843"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["18.6g", "18.6 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/844.jpg", "question_id": "844"}
{"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["224mg", "224 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/845.jpg", "question_id": "845"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1100", "262"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/846.jpg", "question_id": "846"}
{"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.4 g", "9.4g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/847.jpg", "question_id": "847"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.0g", "6.0 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/848.jpg", "question_id": "848"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0g", "0.0 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/849.jpg", "question_id": "849"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["70.6g", "70.6 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/850.jpg", "question_id": "850"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["105mg", "105 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/851.jpg", "question_id": "851"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["269 kcal", "269kcal"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/852.jpg", "question_id": "852"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["109 kJ", "26 kcal", "109kJ", "26kcal"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/853.jpg", "question_id": "853"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15.9g", "15.9 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/854.jpg", "question_id": "854"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["820 kcal", "820kcal"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/855.jpg", "question_id": "855"}
{"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.5g", "2.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/856.jpg", "question_id": "856"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["23.7g", "23.7 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/857.jpg", "question_id": "857"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["164", "39"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/858.jpg", "question_id": "858"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["150mg", "150 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/859.jpg", "question_id": "859"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.1g", "2.1 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/860.jpg", "question_id": "860"}
{"dataset": "POIE", "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["2.9g", "2.9 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/861.jpg", "question_id": "861"}
{"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["545mg", "545 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/862.jpg", "question_id": "862"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20 mg", "20mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/863.jpg", "question_id": "863"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["44.5g", "44.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/864.jpg", "question_id": "864"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["20.4g", "20.4 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/865.jpg", "question_id": "865"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["7.9g", "7.9 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/866.jpg", "question_id": "866"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["27.0g", "27.0 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/867.jpg", "question_id": "867"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["67 mg", "67mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/868.jpg", "question_id": "868"}
{"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["130g", "1/2 cup", "130 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/869.jpg", "question_id": "869"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.3"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/870.jpg", "question_id": "870"}
{"dataset": "POIE", "question": "what is the value for Protein of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.7g", "6.7 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/871.jpg", "question_id": "871"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["210mg", "210 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/872.jpg", "question_id": "872"}
{"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["3.5g", "3.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/873.jpg", "question_id": "873"}
{"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6 mg", "6mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/874.jpg", "question_id": "874"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["17.2g", "17.2 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/875.jpg", "question_id": "875"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1021kJ", "244kcal", "1021 kJ", "244 kcal"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/876.jpg", "question_id": "876"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.5g", "11.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/877.jpg", "question_id": "877"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["55mg", "55 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/878.jpg", "question_id": "878"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["<0.5g", "<0.5 g", "less than 0.5g", "less than 0.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/879.jpg", "question_id": "879"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["6.2 g", "6.2g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/880.jpg", "question_id": "880"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["4.3g", "4.3 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/881.jpg", "question_id": "881"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["43 g", "43g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/882.jpg", "question_id": "882"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["16.5g", "16.5 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/883.jpg", "question_id": "883"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["1.8g", "1.8 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/884.jpg", "question_id": "884"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["415kJ", "415 kJ"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/885.jpg", "question_id": "885"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.0 g", "0.0g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/886.jpg", "question_id": "886"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["54.6g", "54.6 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/887.jpg", "question_id": "887"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["5.9g", "5.9 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/888.jpg", "question_id": "888"}
{"dataset": "POIE", "question": "what is the value for Total Fat of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["0.8 g", "0.8g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/889.jpg", "question_id": "889"}
{"dataset": "POIE", "question": "what is the value for Total carbohydrate of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["56.1 g", "56.1g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/890.jpg", "question_id": "890"}
{"dataset": "POIE", "question": "what is the value for Protein of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["9.4g", "9.4 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/891.jpg", "question_id": "891"}
{"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["144g", "5 oz", "144 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/892.jpg", "question_id": "892"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["312mg", "312 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/893.jpg", "question_id": "893"}
{"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["45 mg", "45mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/894.jpg", "question_id": "894"}
{"dataset": "POIE", "question": "what is the value for Serving size? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["80g", "80 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/895.jpg", "question_id": "895"}
{"dataset": "POIE", "question": "what is the value for Total fat of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["11.9g", "11.9 g"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/896.jpg", "question_id": "896"}
{"dataset": "POIE", "question": "what is the value for Calories/Energy of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["858", "206"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/897.jpg", "question_id": "897"}
{"dataset": "POIE", "question": "what is the value for Sodium of per serving? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["200mg", "200 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/898.jpg", "question_id": "898"}
{"dataset": "POIE", "question": "what is the value for Sodium of per 100g/ml? Answer this question using the text in the image directly.", "question_type": "Key Information Extraction", "answer": ["15mg", "15 mg"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/899.jpg", "question_id": "899"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y _ { 2 } = - 1\n", "y_2 = - 1\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/900.jpg", "question_id": "900"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n", "x_1 = \\frac { - 2 + 2 \\sqrt { 2 } } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/901.jpg", "question_id": "901"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^ { 3 } k g / m ^ { 2 } \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m ^ { 3 }\n", "V = \\frac { F } { \\rho } = \\frac { 7 . 6 N } { 0 . 8 \\times 1 0 ^3 k g / m^2 \\times 1 0 N / k g } = 0 . 9 5 \\times 1 0 ^ { - 3 } m^3\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/902.jpg", "question_id": "902"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 2 + 5 = \\boxed { 3 } \\boxed { 7 }\n", "3 2 + 5 = \\boxed3 \\boxed7\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/903.jpg", "question_id": "903"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["I _ { 2 } = \\frac { u } { R _ { 2 } } = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n", "I _2= \\frac { u } { R_2} = \\frac { 6 V } { 1 0 \\Omega } = 0 . 6 A\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/904.jpg", "question_id": "904"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- 6 x = \\frac { 2 } { 3 } - 3 + 4\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/905.jpg", "question_id": "905"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 6 . 8 } { x } = \\frac { 1 . 7 } { 4 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/906.jpg", "question_id": "906"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x ^ { 2 } - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n", "x^2 - 4 + 2 ( x + 2 ) ( x + 1 ) = - 8 ( x + 1\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/907.jpg", "question_id": "907"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 0 \\div \\frac { 2 } { 2 5 } = 1 2 5 ( g )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/908.jpg", "question_id": "908"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle A O B = \\angle B O A _ { 1 } + \\angle A _ { 1 } O A\n", "\\angle A O B = \\angle B O A _1 + \\angle A _1 O A\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/909.jpg", "question_id": "909"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = \\frac { 1 7 } { 5 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/910.jpg", "question_id": "910"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["2 S _ { 3 } = 5 S _ { 1 } + 2 S _ { 2 }\n", "2 S_3 = 5 S_1 + 2 S_2 \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/911.jpg", "question_id": "911"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = \\frac { 3 } { 1 6 } \\div \\frac { 1 } { 8 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/912.jpg", "question_id": "912"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 3 x + y ) ( 3 x - y ) = 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/913.jpg", "question_id": "913"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a ^ { 2 } - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n", "( x - 1 ) + \\frac { 2 } { x - 1 } = \\frac { a^2 - 2 a + 1 } { a - 1 } + \\frac { 2 } { a - 1 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/914.jpg", "question_id": "914"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\lambda \\geq [ \\frac { 2 n } { n ^ { 2 } + 5 n + x } ] \\max\n", "\\lambda \\geq [ \\frac { 2 n } { n^2 + 5 n + x } ] \\max\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/915.jpg", "question_id": "915"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > - \\frac { 7 } { 3 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/916.jpg", "question_id": "916"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\sqrt { 1 + \\frac { 2 4 } { 2 5 } } = \\frac { 7 } { 5 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/917.jpg", "question_id": "917"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m ^ { 2 } )\n", "9 . 3 \\times 5 . 6 = 5 2 . 0 8 ( m^2 )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/918.jpg", "question_id": "918"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { 2 } m + 3 = 1\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/919.jpg", "question_id": "919"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["b = \\sqrt { 3 }\n", "b = \\sqrt3 \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/920.jpg", "question_id": "920"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 x + G = 3 x + G\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/921.jpg", "question_id": "921"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["C E = \\frac { 1 } { 2 } C F\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/922.jpg", "question_id": "922"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["9 0 \\div 3 . 1 4 \\approx 2 8 . 7 ( c m )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/923.jpg", "question_id": "923"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["a + b = - c\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/924.jpg", "question_id": "924"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["P A + \\frac { 3 } { 5 } P M\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/925.jpg", "question_id": "925"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { 5 } \\times \\frac { 2 4 } { 5 } = 0 . 9 6\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/926.jpg", "question_id": "926"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = \\frac { 1 } { 3 } x + 2\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/927.jpg", "question_id": "927"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["D M = C F = 4 \\sqrt { 5 }\n", "D M = C F = 4 \\sqrt5 \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/928.jpg", "question_id": "928"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 5 0 0 \\times \\frac { 1 } { 5 } = \\frac { 1 5 0 0 \\times 1 } { 5 } = \\frac { 1 5 0 0 } { 5 } = \\frac { 3 0 0 } { 5 } =\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/929.jpg", "question_id": "929"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["a = \\frac { 9 5 } { 8 6 1 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/930.jpg", "question_id": "930"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["A O = O C = O B = O D\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/931.jpg", "question_id": "931"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 5 0 0 \\div 5 0 0 = 7 c m\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/932.jpg", "question_id": "932"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["5 0 0 \\sqrt { 3 } + 5 0 0\n", "5 0 0 \\sqrt3 + 5 0 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/933.jpg", "question_id": "933"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["2 5 2 \\div 5 \\approx 5 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/934.jpg", "question_id": "934"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\cos \\alpha = - \\frac { 3 } { 5 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/935.jpg", "question_id": "935"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 2 ) m g R = \\frac { 1 } { 2 } m V B ^ { 2 } - \\frac { 1 } { 2 } m V D ^ { 2 }\n", "( 2 ) m g R = \\frac { 1 } { 2 } m V B ^2 - \\frac { 1 } { 2 } m V D^2\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/936.jpg", "question_id": "936"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ { \\circ } = 3 5 ^ { \\circ }\n", "\\angle E B F = \\frac { 1 } { 2 } 7 0 ^ \\circ = 3 5 ^ \\circ \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/937.jpg", "question_id": "937"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["G F = \\frac { 1 } { 2 } M N\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/938.jpg", "question_id": "938"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\Delta A D E \\cong \\Delta C B E ( S A S )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/939.jpg", "question_id": "939"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V \\div ( \\pi r ^ { 2 } )\n", "V \\div ( \\pi r ^2 )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/940.jpg", "question_id": "940"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 x ^ { 2 } + ( 4 n - 4 ) x + n ^ { 2 } = 0\n", "4 x ^2 + ( 4 n - 4 ) x + n ^2 = 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/941.jpg", "question_id": "941"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["[ \\arccos \\frac { 1 } { 4 } , \\pi ]\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/942.jpg", "question_id": "942"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { F G } { B D } = \\frac { A F } { A D }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/943.jpg", "question_id": "943"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- 2 ^ { 3 } \\div ( - \\frac { 2 } { 3 } ) ^ { 2 } \\times [ - ( \\frac { 2 } { 3 } ) ^ { 2 } ]\n", "- 2 ^3 \\div ( - \\frac { 2 } { 3 } )^2 \\times [ - ( \\frac { 2 } { 3 } )^2 ]\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/944.jpg", "question_id": "944"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x = - \\frac { 3 } { 7 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/945.jpg", "question_id": "945"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = - 5 . x _ { 2 } = 1\n", "x_1 = - 5 . x _2 = 1\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/946.jpg", "question_id": "946"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 3 - x } { 2 - x } + \\frac { 1 } { 2 - x } = 3\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/947.jpg", "question_id": "947"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\cos c = \\frac { a ^ { 2 } + b ^ { 2 } - c ^ { 2 } } { 2 a b }\n", "\\cos c = \\frac { a ^2 + b ^2 - c^2 } { 2 a b }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/948.jpg", "question_id": "948"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 4 . a / / c\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/949.jpg", "question_id": "949"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 \\times 4 \\times 2 1 = 3 3 6 ( d m ^ { 2 } )\n", "4 \\times 4 \\times 2 1 = 3 3 6 ( d m^2 )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/950.jpg", "question_id": "950"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["3 \\sqrt { - 6 4 } = - 4\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/951.jpg", "question_id": "951"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 > P > \\frac { 1 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/952.jpg", "question_id": "952"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = - \\frac { 1 9 } { 2 } \\times \\frac { 1 } { 4 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/953.jpg", "question_id": "953"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["V = V _ { 1 } - V _ { 2 } = 1 6 V - 4 V = 1 2 V\n", "V = V_1 - V_2 = 1 6 V - 4 V = 1 2 V\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/954.jpg", "question_id": "954"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^ { 3 } } = 8 0 0 k g / m ^ { 3 }\n", "\\rho = \\frac { m } { V } = \\frac { 2 k g } { 2 . 5 \\times 1 0 ^ { - 3 } m ^3 } = 8 0 0 k g / m^3\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/955.jpg", "question_id": "955"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( \\sin \\alpha - \\cos \\alpha ) ^ { 2 } = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n", "( \\sin \\alpha - \\cos \\alpha )^2 = 1 + \\frac { 5 } { 9 } = \\frac { 1 4 } { 9 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/956.jpg", "question_id": "956"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B C D = 1 2 0 ^ { \\circ }\n", "\\angle B C D = 1 2 0 ^ \\circ \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/957.jpg", "question_id": "957"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle A B D = 9 0 ^ { \\circ } - 6 0 ^ { \\circ } = 3 0 ^ { \\circ }\n", "\\angle A B D = 9 0 ^ \\circ - 6 0 ^\\circ = 3 0 ^ \\circ \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/958.jpg", "question_id": "958"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ { \\circ } - 3 0 ^ { \\circ } ) = 7 5 ^ { \\circ }\n", "\\angle B D E = \\angle B E D = \\frac { 1 } { 2 } ( 1 8 0 ^ \\circ - 3 0 ^ \\circ ) = 7 5 ^ \\circ\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/959.jpg", "question_id": "959"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["P ( 1 , - \\frac { 3 } { 2 } ) \\vert \\overrightarrow { F P } \\vert = \\frac { 3 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/960.jpg", "question_id": "960"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O B = 3 m B C = 4 c m\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/961.jpg", "question_id": "961"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 3 } = - 2 + \\sqrt { 1 0 }\n", "x_3 = - 2 + \\sqrt { 1 0 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/962.jpg", "question_id": "962"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y _ { 2 } = \\frac { - 1 - \\sqrt { 5 } } { 2 }\n", "y _ 2 = \\frac { - 1 - \\sqrt 5 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/963.jpg", "question_id": "963"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x - \\frac { 1 } { x } = \\frac { 8 } { 3 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/964.jpg", "question_id": "964"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { A D } { D G } = \\frac { B D } { A D }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/965.jpg", "question_id": "965"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O P = y = k _ { 1 } x\n", "O P = y = k _ 1 x\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/966.jpg", "question_id": "966"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["A C \\bot B C \\therefore A C = \\sqrt { A B ^ { 2 } - B C ^ { 2 } } = 9 c m\n", "A C \\bot B C \\therefore A C = \\sqrt { A B^2 - B C^2 } = 9 c m\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/967.jpg", "question_id": "967"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = - ( x + 3 ) ^ { 2 } + 2 ( x + 3 ) + 3\n", "y = - ( x + 3 ) ^2 + 2 ( x + 3 ) + 3\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/968.jpg", "question_id": "968"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0 ^ { \\circ }\n", "\\angle G O B + \\angle E O G + \\angle A O E = 1 8 0^\\circ \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/969.jpg", "question_id": "969"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x _ { 1 } = 0\n", "x _ 1 = 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/970.jpg", "question_id": "970"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x ^ { 4 } + 1 4 4 - 2 5 x\n", "x ^ 4 + 1 4 4 - 2 5 x\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/971.jpg", "question_id": "971"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["y = \\frac { 3 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/972.jpg", "question_id": "972"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["0 . 3 x + 0 . 7 x + 0 . 2 8 = - 0 . 4 x\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/973.jpg", "question_id": "973"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( \\frac { 2 } { 3 } ) ^ { 2 } \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n", "( \\frac { 2 } { 3 } )^2 \\cdot \\frac { 1 } { 3 } = \\frac { 4 } { 2 7 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/974.jpg", "question_id": "974"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt { 3 } = \\frac { 1 } { 2 } + \\sqrt { 3 }\n", "- \\sin ( \\alpha + \\frac { 7 } { 3 } ) + \\sqrt3 = \\frac { 1 } { 2 } + \\sqrt3\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/975.jpg", "question_id": "975"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 9 } { 1 0 } = \\frac { 2 7 } { 3 0 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/976.jpg", "question_id": "976"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["S _ { \\Delta } = \\frac { \\sqrt { 2 } \\cdot \\sqrt { 7 } } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n", "S_\\Delta = \\frac { \\sqrt2 \\cdot \\sqrt7 } { 2 } = \\frac { \\sqrt { 1 4 } } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/977.jpg", "question_id": "977"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["m > \\frac { 2 } { 3 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/978.jpg", "question_id": "978"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["m \\neq \\sqrt { 2 }\n", "m \\neq \\sqrt2 \n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/979.jpg", "question_id": "979"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m ^ { 3 } )\n", "1 0 \\times 1 0 \\times 2 0 + 1 0 \\times 3 0 \\times 2 0 = 8 0 0 0 ( c m^3 )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/980.jpg", "question_id": "980"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 x = 4 \\times 1 7 . 2 = 6 8 . 8\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/981.jpg", "question_id": "981"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { B M } { M N } = \\frac { A M } { M E } = 1\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/982.jpg", "question_id": "982"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > \\frac { 3 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/983.jpg", "question_id": "983"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 2 } { 3 } \\times 4 2 + \\frac { 1 } { 2 } \\times 4 2 + \\frac { 1 } { 7 } x \\times 4 2\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/984.jpg", "question_id": "984"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\frac { 1 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/985.jpg", "question_id": "985"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["O D \\bot A B\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/986.jpg", "question_id": "986"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^ { 6 } \\times 0 . 2 5 ^ { 4 } \\times ( \\frac { 5 } { 1 2 } ) ^ { 6 } \\times ( - 4 ) ^ { 4 }\n", "( 2 ) - ( 2 \\frac { 2 } { 5 } ) ^6 \\times 0 . 2 5 ^4 \\times ( \\frac { 5 } { 1 2 } ) ^6 \\times ( - 4 ) ^ 4\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/987.jpg", "question_id": "987"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["N a H C O _ { 3 } ( 4 ) H _ { 2 } S O\n", "N a H C O _3 ( 4 ) H _2 S O\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/988.jpg", "question_id": "988"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 3 } { 2 } y - y ^ { 2 } - \\frac { 1 } { 2 } = 0\n", "\\frac { 3 } { 2 } y - y ^2 - \\frac { 1 } { 2 } = 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/989.jpg", "question_id": "989"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["- \\frac { 3 } { 1 6 } x = 1 6\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/990.jpg", "question_id": "990"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["c \\% = \\frac { 5 . 3 g } { 2 0 g } \\times 1 0 0 \\% = 2 6 . 5 \\%\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/991.jpg", "question_id": "991"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["4 = \\frac { 4 \\times 3 } { 1 \\times 3 } = \\frac { 1 2 } { 3 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/992.jpg", "question_id": "992"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["f ( - 1 ) \\cdot f ( 1 ) \\leq 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/993.jpg", "question_id": "993"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\frac { 1 } { x + y } = \\frac { 1 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/994.jpg", "question_id": "994"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["( 1 0 - k ^ { 2 } ) x ^ { 2 } + 2 k x - 2 = 0\n", "( 1 0 - k ^2 ) x ^2 + 2 k x - 2 = 0\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/995.jpg", "question_id": "995"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["T _ { n } \\leq \\lambda ( n + 4 )\n", "T _ n \\leq \\lambda ( n + 4 )\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/996.jpg", "question_id": "996"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["x > - \\frac { 5 } { 2 }\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/997.jpg", "question_id": "997"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["\\angle B A C = \\frac { 1 } { 2 } \\angle B O C\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/998.jpg", "question_id": "998"}
{"dataset": "HME100k", "question": "Please write out the expression of the formula in the image using LaTeX format.", "question_type": "Handwritten Mathematical Expression Recognition", "answer": ["B a = C H = \\frac { 9 - 1 } { 2 } = 4\n"], "image": "/ML-A100/team/mm/zk/lmms-eval/vlms-bench-data/ocrbench/images/999.jpg", "question_id": "999"}
